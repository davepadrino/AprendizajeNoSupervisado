---
title: "Tarea 3"
author: "David Padrino"
date: "Domingo, 10 de abril de 2016"
widescreen: yes
runtime: shiny
output: html_document
---
# Introducción
Primero que nada se debe establecer el _location directory_ como la bicación actual del archivo.
Se instalan e invocan las bibliotecas necesarias, se leen los archivos a medida que se va trabajando con ellos
```{r setup}  
# install.packages("clue")
# install.packages("Rcmdr")
# install.packages("rglwidget")
library("stringr")
library("caret")
library("clue")
library("cluster")
library("rgl")
library("knitr")
library("rglwidget")
knit_hooks$set(webgl = hook_webgl)
```

Se procede a crear 2 arreglos con los metodos necesarios para realizar el clustering jerárquico.

```{r}
hclust_methods <- c("ward.D", "single", "complete", "average", "mcquitty", "median", "centroid", "ward.D2")
dist_methods <- c("euclidean", "maximum", "manhattan", "minkowski")
```

Se crearon ciertas funciones por comodidad, como la implementación del codo de jambú, calculo de kmedias para 2D y 3D, comparación de Kmedias y PAM a través de sus matries de confusión, etc.
```{r, echo=FALSE}
#Set classes after 1
set_class = function(dataset){
  dataset$class <-as.numeric(dataset$class)
  dataset$class <- dataset$class + 1
  return(dataset)
}

# codo de jambu 
c.jambu= function(d){
  mydata <- d
  wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
  for (i in 2:15) wss[i] <- sum(kmeans(mydata, centers=i)$withinss)
  plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
}

# Calculating best kmeans algorithm 2D
k.means2D = function(dataset, centers){
  kmeans.accuracy.CM <- 0
  kmeans.better.accuracy.CM <- 0
  for (i in 1:10){
    kmeans <- kmeans(dataset[,c("x", "y")], centers = centers)
    kmeans.CM <- table(kmeans$cluster, dataset$class)
    kmeans.accuracy.CM <- sum(diag(kmeans.CM))/sum(kmeans.CM)
    if (kmeans.accuracy.CM > kmeans.better.accuracy.CM){
      kmeans.better <- kmeans
      kmeans.better.accuracy.CM <- kmeans.accuracy.CM # useless (?)
    }
  }
  return(kmeans.better)
}


# Calculating best kmeans algorithm 2D
k.means3D = function(dataset, centers){
  kmeans.accuracy.CM <- 0
  kmeans.better.accuracy.CM <- 0
  for (i in 1:10){
    kmeans <- kmeans(dataset[,c("x", "y","z")], centers = centers)
    kmeans.CM <- table(kmeans$cluster, dataset$class)
    kmeans.accuracy.CM <- sum(diag(kmeans.CM))/sum(kmeans.CM)
    if (kmeans.accuracy.CM > kmeans.better.accuracy.CM){
      kmeans.better <- kmeans
      kmeans.better.accuracy.CM <- kmeans.accuracy.CM # useless (?)
    }
  }
  return(kmeans.better)
}


# Comparing kmeans vs PAM
compare.kmeans.pam = function(kmeans.acc, pam.acc){
  kmeans.vs.pam <- 0
  if (kmeans.acc >= pam.acc){
    #a.kmeans.vs.pam <- kmeans.acc
    kmeans.vs.pam <- c("kmeans", kmeans.acc)
    return(kmeans.vs.pam) 
  }else{
    #a.kmeans.vs.pam <- pam.acc
    kmeans.vs.pam <- c("pam", pam.acc)
    return(kmeans.vs.pam) 
  }
}

# Calculating each distance method vs each hclust method 
match.hclust = function(matrix, k, dataset){
  better.accuracy <- 0
  for (i in 1:length(dist_methods)){
    for (j in 1:length(hclust_methods)){
      dist.mat <- dist(matrix, method = dist_methods[i]) # distance matrix
      cluster <- hclust(dist.mat, method = hclust_methods[j]) # apply method
      ct <- cutree(cluster, k) # k to generate k clusters
      CM <- table(as.factor(dataset$class), as.factor(ct))
      accuracy.CM <- sum(diag(CM))/sum(CM)
      if (accuracy.CM > better.accuracy){
        better.accuracy <- accuracy.CM
        better <- c(dist_methods[i], hclust_methods[j], accuracy.CM)
      }
    }
  }
  return(better)
}
```

## Archivo *a.csv*
```{r}
## Exploratory Analysis
a <- read.csv("a.csv")
# dim(a)
names(a)
# str(a)
summary(a)
```
Se observa que la ultima columna efectivamente es de números enteros lo cual sirve para establecer las clases, se procede a cambiar los nombres de las columnas y a realizar un gráfico con los colores de los elementos para apreciar mejor la distribucion de los mismos.

```{r}
names(a)[1] <- "x"
names(a)[2] <- "y"
names(a)[3] <- "class"
plot(a$x, a$y, col=1:3) 
```
Al apreciar el gráfico se prueba con 3 centroides.
Luego se procede al cálculo de K-medias y PAM respectivamente (Se utilizó PAM, que es un algoritmo de K-medioides, el cual es bueno pero su solución no es la mejor debido a que no realiza una búsqueda exahustiva, lo cual es su mayor ventaja ya que es mucho mas rápido.)

Ambos algoritmos se corrieron con 3 centros y medioides respectivamente.

**Observación:** La diferencia entre medioide y centroide es que los medioides forman parte del Dataset obligatoriamente, los centroides no necesariamente.

Los resultados de las matrices de confusion se hacen entre PAM y K-medias de forma que el "ganador" se compare con el mejor algoritmo de cluster jerárquico.

```{r, echo=F}
## K means
a.kmeans <- k.means2D(dataset = a, centers =3)
a.kmeans.CM <- table(a.kmeans$cluster, a$class)
a.kmeans.accuracy <- sum(diag(a.kmeans.CM))/sum(a.kmeans.CM)


## Partitioning Around Medioids (PAM)
a.pam <- pam(a[,1:2], 3)
a.pam.CM <- table(a.pam$clustering, a$class)
a.pam.accuracy.CM <- sum(diag(a.pam.CM))/sum(a.pam.CM)

#### Comparison of kmean's centroids vs pam's medioids
a.kmeans.vs.pam <- compare.kmeans.pam(a.kmeans.accuracy, a.pam.accuracy.CM)

# Medioids are part of the DataSet, centroids aren't necessarily
```
Luego se realiza el clustering jerárquico. Se utilizaron los arreglos de los métodos para hacer un amtch y asi elegir el mejor de ellos.


```{r, echo=F}
## Hcluster
a.num <- a # a copy of the dataframe
a.num$class <-NULL # Delete class column
a.num <- as.matrix(a.num) # convert into a matrix

# Calculating best hclust method 
a.hclust = match.hclust(a.num, k = 3, a)
```

Se realiza la comparación y el resultante se muestra a continuación
```{r,  echo=F}
# Comparison hclust vs kmeans.vs.pam
if (a.hclust[3] > a.kmeans.vs.pam[2]){
  a.final.cluster <- a.hclust[3]
  a.dist.mat <- dist(a.num, method = a.hclust[1]) # distance matrix
  a.cluster <- hclust(a.dist.mat, method = a.hclust[2]) # apply method
  a.ct <- cutree(a.cluster, k =3) # k to generate 3 clusters
  a.dendrogram <- as.dendrogram(a.cluster)
  plot(a.dendrogram) # dendrogram
  rect.hclust(a.cluster, k = 3, border = c("red"))
  a.corte <- cut(a.dendrogram, h=20)$upper # $upper to get useful information instead a forest
  plot(a.corte)
  plot(a$x, a$y, col= a.ct, main = "HCluster")
}else{
  a.final.cluster <- a.kmeans.vs.pam
  if (a.kmeans.vs.pam[1] == 'pam'){
    plot(a$x, a$y, col = a.pam$clustering, main = "PAM")
    points(a.pam$medoids,
           col=1:3,
           pch = 18,
           cex = 3)
  }else{
    plot(a$x, a$y, col= a.kmeans$cluster, main = "K-means")
    points(a.kmeans$centers[, c("x", "y")],
           col=1:3,
           pch = 19,
           cex = 3)
  }
}


```

Con una exactitud de:
```{r,  echo=F}
a.final.cluster
```



## Archivo *moon.csv*
```{r}
## Exploratory Analysis
moon <- read.csv("moon.csv")
# dim(moon)
names(moon)
# str(moon)
summary(moon)
```
Se observa que la ultima columna efectivamente es de números enteros lo cual sirve para establecer las clases, se procede a cambiar los nombres de las columnas y a realizar un gráfico con los colores de los elementos para apreciar mejor la distribucion de los mismos.

```{r}
names(moon)[1] <- "x"
names(moon)[2] <- "y"
names(moon)[3] <- "class"
plot(moon$x, moon$y, col=1:3) 
```
Al apreciar el gráfico se prueba con 2 centroides.
Luego se procede al cálculo de K-medias y PAM respectivamente 

Ambos algoritmos se corrieron con 2 centros y medioides respectivamente.

Los resultados de las matrices de confusion se hacen entre PAM y K-medias de forma que el "ganador" se compare con el mejor algoritmo de cluster jerárquico.

```{r, echo=F}
## K means
moon.kmeans <- k.means2D(dataset = moon, centers = 2)
moon.kmeans.CM <- table(moon.kmeans$cluster, moon$class)
moon.kmeans.accuracy <- sum(diag(moon.kmeans.CM))/sum(moon.kmeans.CM)


## Partitioning Around Medioids (PAM)
moon.pam <- pam(moon[,1:2], 2)
moon.pam.CM <- table(moon.pam$clustering, moon$class)
moon.pam.accuracy.CM <- sum(diag(moon.pam.CM))/sum(moon.pam.CM)

#### Comparison of kmean's centroids vs pam's medioids
moon.kmeans.vs.pam <- compare.kmeans.pam(moon.kmeans.accuracy, moon.pam.accuracy.CM)

# Medioids are part of the DataSet, centroids aren't necessarily
```
Luego se realiza el clustering jerárquico. Se utilizaron los arreglos de los métodos para hacer un amtch y asi elegir el mejor de ellos.


```{r, echo=F}
## Hcluster
moon.num <- moon # a copy of the dataframe
moon.num$class <-NULL # Delete class column
moon.num <- as.matrix(moon.num) # convert into a matrix

# Calculating best hclust method 

moon.hclust <- match.hclust(moon.num, 2, moon)
```

Se realiza la comparación y el resultante se muestra a continuación
```{r,  echo=F}
# Comparison hclust vs kmeans.vs.pam
if (moon.hclust[3] > moon.kmeans.vs.pam[2]){
  moon.final.cluster <- moon.hclust[3]
  moon.dist.mat <- dist(moon.num, method = moon.hclust[1]) # distance matrix
  moon.cluster <- hclust(moon.dist.mat, method = moon.hclust[2]) # apply method
  moon.ct <- cutree(moon.cluster, k =2) # k to generate 3 clusters
  moon.dendrogram <- as.dendrogram(moon.cluster)
  plot(moon.dendrogram, main = "Dendrograma de HClust") # dendrogram
  rect.hclust(moon.cluster, k = 2, border = c("cyan"))
  moon.corte <- cut(moon.dendrogram, h=20)$upper # $upper to get useful information instead a forest
  plot(moon.corte, main = "Dendrograma cortado para dos clusters")
  plot(moon$x, moon$y, col= moon.ct, main = "HCluster")
}else{
  moon.final.cluster <- moon.kmeans.vs.pam
  if (moon.kmeans.vs.pam[1] == 'pam'){
    plot(moon$x, moon$y, col = moon.pam$clustering, main = "PAM")
    points(moon.pam$medoids,
           col=1:2,
           pch = 18,
           cex = 3)
  }else{
    plot(moon$x, moon$y, col= moon.kmeans$cluster, main = "K-means")
    points(moon.kmeans$centers[, c("x", "y")],
           col=1:2,
           pch = 19,
           cex = 3)
  }
}
```

En donde los métodos seleccionados para la matriz de distancia fue
```{r,  echo=F}
moon.hclust[1]
```

y para el método del clustering jerárquico:
```{r,  echo=F}
moon.hclust[2]
```

Con una exactitud de:
```{r,  echo=F}
moon.final.cluster
```

## Archivo *h.csv*
```{r, echo=FALSE}
h.clase = function(numero){
  # Selecting 3 clusters
  if(numero < 6.0)
    return(1)
  else if(numero < 8.0)
    return(2)
  else if(numero < 10.0)
    return(3)
  else if(numero < 12.0)
    return(4)
  else
    return(5)
}
```


```{r}
## Exploratory Analysis
h <- read.csv("h.csv")
#dim(h)
names(h)
#str(h)
summary(h)
# c.jambu(h)
```

Se observa que la ultima columna efectivamente es de números flotantes lo cual no sirve para establecer las clases, se procede a realizar una funcion de asignacion de clases, luego a cambiar los nombres de las columnas y finalmente realizar un gráfico con los colores (nuevas clases asignadas con la funcion) para apreciar mejor la distribucion de los datos

```{r, testgl0, webgl=TRUE}

names(h)[1] <- "x"
names(h)[2] <- "y"
names(h)[3] <- "z"
names(h)[4] <- "class"
for (i in 1:length(h$class)){
  h$class[i] <- h.clase(h$class[i])
}

plot3d(h$x, h$y, h$z, col = h$class, main="Vista Previa")


```


Se eligieron 5 clusters por la forma del dataset, a medida que el "espiral" se va "desenrollando" da la impresion que tambien se van separando los puntos del conglomerado inicial
Además se eligio un numero entero menor y mas cercano al primer cuartil, mismo para el 3er cuartil

Al apreciar el gráfico se prueba con 5 centroides.
Luego se procede al cálculo de K-medias y PAM respectivamente 

Ambos algoritmos se corrieron con 5 centros y medioides respectivamente.

Los resultados de las matrices de confusion se hacen entre PAM y K-medias de forma que el "ganador" se compare con el mejor algoritmo de cluster jerárquico.

```{r, echo=F}
## K means
h.kmeans <- k.means3D(dataset = h, centers = 5)
h.kmeans.CM <- table(h.kmeans$cluster, h$class)
h.kmeans.accuracy <- sum(diag(h.kmeans.CM))/sum(h.kmeans.CM)


## Partitioning Around Medioids (PAM)
h.pam <- pam(h[,1:3], 5)
h.pam.CM <- table(h.pam$clustering, h$class)
h.pam.accuracy.CM <- sum(diag(h.pam.CM))/sum(h.pam.CM)

#### Comparison of kmean's centroids vs pam's medioids
h.kmeans.vs.pam <- compare.kmeans.pam(h.kmeans.accuracy, h.pam.accuracy.CM)

# Medioids are part of the DataSet, centroids aren't necessarily
```
Luego se realiza el clustering jerárquico. Se utilizaron los arreglos de los métodos para hacer un amtch y asi elegir el mejor de ellos.


```{r, echo=F}
## Hcluster
h.num <- h # a copy of the dataframe
h.num$class <-NULL # Delete class column
h.num <- as.matrix(h.num) # convert into a matrix

# Calculating each distance method vs each hclust method 
h.hclust = match.hclust(h.num, 5, h)
```

Se realiza la comparación y el resultante se muestra a continuación
```{r,  echo=F,testgl1, webgl=TRUE}
# Comparison hclust vs kmeans.vs.pam
if (h.hclust[3] > h.kmeans.vs.pam[2]){
  h.final.cluster <- h.hclust[3]
  h.dist.mat <- dist(h.num, method = h.hclust[1]) # distance matrix
  h.cluster <- hclust(h.dist.mat, method = h.hclust[2]) # apply method
  h.ct <- cutree(h.cluster, k =5) # k to generate 3 clusters
  h.dendrogram <- as.dendrogram(h.cluster)
  plot(h.dendrogram) # dendrogram
  rect.hclust(h.cluster, k = 5, border = c("cyan"))
  h.corte <- cut(h.dendrogram, h=205)$upper # $upper to get useful information instead a forest
  plot(h.corte)
  #rgl.open()
  #rgl.bg(color = "white") # Setup the background color
  plot3d(h$x, h$y, h$z, col = h.ct, main = "HCluster")
}else{
  h.final.cluster <- h.kmeans.vs.pam
  if (h.kmeans.vs.pam[1] == 'pam'){
    #rgl.open()
    #rgl.bg(color = "white") # Setup the background color
    plot3d(h$x, h$y, h$z, col = h.pam$clustering, main = "PAM")
    rgl.spheres(h.pam$medoids[, c("x", "y", "z")], r = 0.4, color = 1:5) 
  }else{
    #rgl.open()
    #rgl.bg(color = "white") # Setup the background color
    plot3d(h$x, h$y, h$z, col = h.kmeans$cluster, main = "K-means")
    rgl.spheres(h.kmeans$centers[, c("x", "y", "z")], r = 0.4, color = 1:5) 
  }
}

```

Con una exactitud de:
```{r,  echo=F}
h.final.cluster
```


## Archivo *s.csv*
```{r, echo=FALSE}
s.clase = function(numero){
  # Selecting 3 clusters
  if(numero < -2.0)
    return(1)
  else if(numero < 0.0)
    return(2)
  else
    return(3)
} 

```


```{r}
## Exploratory Analysis
s <- read.csv("s.csv")
dim(s)
#names(s)
#str(s)
summary(s)
# c.jambu(s)



```

Se observa que la ultima columna es de números flotantes lo cual no sirve para establecer las clases, se procede a realizar una funcion de asignacion de clases, luego a cambiar los nombres de las columnas y finalmente realizar un gráfico con los colores (nuevas clases asignadas con la funcion) para apreciar mejor la distribucion de los datos

```{r, testgl2, webgl=TRUE}

names(s)[1] <- "x"
names(s)[2] <- "y"
names(s)[3] <- "z"
names(s)[4] <- "class"
for (i in 1:length(s$class)){
  s$class[i] <- s.clase(s$class[i])
}

plot3d(s$x, s$y, s$z, col = s$class, main = "Vista Previa")


```


Se eligieron 3 clusters por la forma del dataset, a medida que el "espiral" se va "desenrollando" da la impresion que tambien se van separando los puntos del conglomerado inicial
Además se eligio un numero entero menor y mas cercano al primer cuartil, mismo para el 3er cuartil

Al apreciar el gráfico se prueba con 5 centroides.
Luego se procede al cálculo de K-medias y PAM respectivamente 

Ambos algoritmos se corrieron con 5 centros y medioides respectivamente.

Los resultados de las matrices de confusion se hacen entre PAM y K-medias de forma que el "ganador" se compare con el mejor algoritmo de cluster jerárquico.

```{r, echo=F}
## K means
s.kmeans <- k.means3D(dataset = s, centers = 3)
s.kmeans.CM <- table(s.kmeans$cluster, s$class)
s.kmeans.accuracy <- sum(diag(s.kmeans.CM))/sum(s.kmeans.CM)


## Partitioning Around Medioids (PAM)
s.pam <- pam(s[,1:3], 3)
s.pam.CM <- table(s.pam$clustering, s$class)
s.pam.accuracy.CM <- sum(diag(s.pam.CM))/sum(s.pam.CM)

#### Comparison of kmean's centroids vs pam's medioids
s.kmeans.vs.pam <- compare.kmeans.pam(s.kmeans.accuracy, s.pam.accuracy.CM)

```

Luego se realiza el clustering jerárquico. Se utilizaron los arreglos de los métodos para hacer un amtch y asi elegir el mejor de ellos.


```{r, echo=F}
## Hcluster
s.num <- s # a copy of the dataframe
s.num$class <-NULL # Delete class column
s.num <- as.matrix(s.num) # convert into a matrix

# Calculating each distance method vs each hclust method 
s.hclust <- match.hclust(s.num, 2, s)
```

Se realiza la comparación y el resultante se muestra a continuación
```{r,  echo=F,testgl3, webgl=TRUE}
if (s.hclust[3] > s.kmeans.vs.pam[2]){
  s.final.cluster <- s.hclust[3]
  s.dist.mat <- dist(s.num, method = s.hclust[1]) # distance matrix
  s.cluster <- hclust(s.dist.mat, method = s.hclust[2]) # apply method
  s.ct <- cutree(s.cluster, k =2) # k to generate 4 clusters
  s.dendrogram <- as.dendrogram(s.cluster.better)
  plot(s.dendrogram) # dendrogram
  rect.hclust(s.cluster.better, k = 2, border = c("red"))
  s.corte <- cut(s.dendrogram, h=0.21)$upper # $upper to get useful information instead a forest
  plot(s.corte)
  rgl.open()
  rgl.bg(color = "white") # Setup the background color
  plot3d(s$x, s$y, s$z, col = s.ct, main = "HCluster")
}else{
  s.final.cluster <- s.kmeans.vs.pam
  if (s.kmeans.vs.pam[1] == 'pam'){
    rgl.open()
    rgl.bg(color = "white") # Setup the background color
    plot3d(s$x, s$y, s$z, col = s.pam$clustering, main = "PAM")
    rgl.spheres(s.pam$medoids[, c("x", "y", "z")], r = 0.2, color = 1:3) 
  }else{
    rgl.open()
    rgl.bg(color = "white") # Setup the background color
    plot3d(s$x, s$y, s$z, col = s.kmeans$cluster, main = "K-means")
    rgl.spheres(s.kmeans$centers[, c("x", "y", "z")], r = 0.2, color = 1:3) 
  }
}

```

Con una exactitud de:
```{r,  echo=F}
s.final.cluster
```

## Archivo *guess.csv*
Dado este dataset se implementó el método Codo de Jambú para poder determinar el número correcto de clusters a elegir para realizar nuestras pruebas y modelos.
```{r}
## Exploratory Analysis
guess <- read.csv("guess.csv")
c.jambu(guess)
dim(guess)
# names(guess)
# str(guess)
summary(guess)



```

Se observa que no existe la ultima columna con lo cual se debe deescribir el modelo sin clases, en la gráfica se verá la forma del cluster.

```{r}

names(guess)[1] <- "x"
names(guess)[2] <- "y"
plot(guess$x, guess$y) 
```


Se eligieron 3 clusters por la forma del dataset, a medida que el "espiral" se va "desenrollando" da la impresion que tambien se van separando los puntos del conglomerado inicial
Además se eligio un numero entero menor y mas cercano al primer cuartil, mismo para el 3er cuartil

Al no existir columna _clase_ no se pueden crear matrices de confusión para evaluar la exactitud de un modelo.


Al apreciar el gráfico se prueba con 5 centroides.
Luego se procede al cálculo de K-medias y PAM respectivamente 

Ambos algoritmos se corrieron con 5 centros y medioides respectivamente.


```{r, echo=F}
## K means
guess.kmeans <- kmeans(guess[,c("x", "y")], centers = 5)

plot(guess$x, guess$y, col= guess.kmeans$cluster, main = "K-means")
points(guess.kmeans$centers[, c("x", "y")],
       col=1:5,
       pch = 19,
       cex = 3)

## Partitioning Around Medioids (PAM)
guess.pam <- pam(guess[,1:2], 5)

### ***** plot PAM
plot(guess$x, guess$y, col = guess.pam$clustering, main = "PAM")
points(guess.pam$medoids,
       col=1:5,
       pch = 18,
       cex = 3)
```

Luego se realiza el clustering jerárquico. Se utilizaron los métodos single y euclidean que fueron los mejores que lograron dividir el dataset


```{r, echo=F}
## Hcluster
guess.num <- guess # a copy of the dataframe
guess.num <- as.matrix(guess.num) # convert into a matrix
guess.dist.mat <- dist(guess.num, method = dist_methods[1]) # distance matrix
guess.cluster <- hclust(guess.dist.mat, method = hclust_methods[1]) # apply method
guess.ct <- cutree(guess.cluster, k = 5) # to generate k clusters




#### ****** plot HCLUST
dendrogram <- as.dendrogram(guess.cluster)
plot(dendrogram)
rect.hclust(guess.cluster, k = 5, border = c("red"))
corte <- cut(dendrogram, h=5000)$upper # $upper to get useful information instead a forest
plot(corte)
plot(guess$x, guess$y, col= guess.ct, main = "HCluster")
```

























































