---
title: "Tarea 3"
author: "David Padrino"
date: "Domingo, 10 de abril de 2016"
widescreen: yes
runtime: shiny
output: html_document
---
# Introducción
Primero que nada se debe establecer el _location directory_ como la ubicación actual del archivo.
Se instalan e invocan las bibliotecas necesarias, se leen los archivos a medida que se va trabajando con ellos

#### OJO Definiciones Kmedias, PAM, Hclust, clustering, dendrograma, aprendizaje no supervisado

# Puesta a punto
## 1 - Se cargan las bibliotecas necesarias
Entre ellas, existen bibliotecas que permiten interactuar con objetos, rotar y mover gráficos en 3D, por ejemplo.
```{r setup, message=FALSE, warning=FALSE}  
# install.packages("clue")
# install.packages("Rcmdr")
# install.packages("rglwidget")
library("stringr")
library("caret")
library("clue")
library("cluster")
library("rgl")
library("knitr")
library("rglwidget")
knit_hooks$set(webgl = hook_webgl)
```

## 2 - Se procede a crear 2 arreglos con los metodos necesarios para realizar el clustering jerárquico. 
Se aclara que **para todos los metodos donde se usó clustering jerárquico se utilizó una función personalizada que selecciona la mejor combinación de los métodos tanto de distancia como de clustering expuestos en los arreglos debajo de este párrafo, basado en la tasa de aciertos de su matriz de confusión**




```{r}
hclust_methods <- c("ward.D", "single", "complete", "average", "mcquitty", "median", "centroid", "ward.D2")
dist_methods <- c("euclidean", "maximum", "manhattan", "minkowski")
```

## 3 - Se crearon ciertas funciones por comodidad, como la implementación del codo de jambú, calculo de kmedias para 2D y 3D, comparación de Kmedias y PAM a través de sus matries de confusión, etc.
```{r, echo=FALSE}
#Set classes after 1
set_class = function(dataset){
  dataset$class <-as.numeric(dataset$class)
  dataset$class <- dataset$class + 1
  return(dataset)
}

# codo de jambu 
c.jambu= function(d){
  mydata <- d
  wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
  for (i in 2:15) wss[i] <- sum(kmeans(mydata, centers=i)$withinss)
  plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
}

# Calculating best kmeans algorithm 2D
k.means2D = function(dataset, centers){
  kmeans.accuracy.CM <- 0
  kmeans.better.accuracy.CM <- 0
  for (i in 1:10){
    kmeans <- kmeans(dataset[,c("x", "y")], centers = centers)
    kmeans.CM <- table(kmeans$cluster, dataset$class)
    kmeans.accuracy.CM <- sum(diag(kmeans.CM))/sum(kmeans.CM)
    if (kmeans.accuracy.CM > kmeans.better.accuracy.CM){
      kmeans.better <- kmeans
      kmeans.better.accuracy.CM <- kmeans.accuracy.CM # useless (?)
    }
  }
  return(kmeans.better)
}


# Calculating best kmeans algorithm 2D
k.means3D = function(dataset, centers){
  kmeans.accuracy.CM <- 0
  kmeans.better.accuracy.CM <- 0
  for (i in 1:10){
    kmeans <- kmeans(dataset[,c("x", "y","z")], centers = centers)
    kmeans.CM <- table(kmeans$cluster, dataset$class)
    kmeans.accuracy.CM <- sum(diag(kmeans.CM))/sum(kmeans.CM)
    if (kmeans.accuracy.CM > kmeans.better.accuracy.CM){
      kmeans.better <- kmeans
      kmeans.better.accuracy.CM <- kmeans.accuracy.CM # useless (?)
    }
  }
  return(kmeans.better)
}


# Comparing kmeans vs PAM
compare.kmeans.pam = function(kmeans.acc, pam.acc){
  kmeans.vs.pam <- 0
  if (kmeans.acc >= pam.acc){
    #a.kmeans.vs.pam <- kmeans.acc
    kmeans.vs.pam <- c("kmeans", kmeans.acc)
    return(kmeans.vs.pam) 
  }else{
    #a.kmeans.vs.pam <- pam.acc
    kmeans.vs.pam <- c("pam", pam.acc)
    return(kmeans.vs.pam) 
  }
}

# Calculating each distance method vs each hclust method 
match.hclust = function(matrix, k, dataset){
  better.accuracy <- 0
  for (i in 1:length(dist_methods)){
    for (j in 1:length(hclust_methods)){
      dist.mat <- dist(matrix, method = dist_methods[i]) # distance matrix
      cluster <- hclust(dist.mat, method = hclust_methods[j]) # apply method
      ct <- cutree(cluster, k) # k to generate k clusters
      CM <- table(as.factor(dataset$class), as.factor(ct))
      accuracy.CM <- sum(diag(CM))/sum(CM)
      if (accuracy.CM > better.accuracy){
        better.accuracy <- accuracy.CM
        better <- c(dist_methods[i], hclust_methods[j], accuracy.CM)
      }
    }
  }
  return(better)
}
```
## ** Comienzo de Actividades**
## Archivo *a.csv*

```{r}
## Exploratory Analysis
a <- read.csv("a.csv")
summary(a)
```

Este es el primero de cuatro datasets que tiene su última columna con números enteros, los cuales pueden ser utilizados como clase de los grupos.
Se utiliza la función _set class_ previamente creada para establecer los valores de la ultima columa (entre 0 y 2) entre 1 y 3.
El gráfico de los cluster se muestra a continuación.

```{r}
names(a)[1] <- "x"
names(a)[2] <- "y"
names(a)[3] <- "class"
a <- set_class(a)
plot(a$x, a$y, col=a$class, main = "Vista Previa")
```


Se eligieron 3 clusters por la forma que presenta el dataset, el cual presenta 3 grupos bien definidos.

Luego se procede al cálculo de K-medias y PAM con 3 centros y medioides respectivamente.


### K-medias
```{r, echo=F}
## K means
a.kmeans <- k.means2D(dataset = a, centers =3)
a.kmeans.CM <- table(a.kmeans$cluster, a$class)
a.kmeans.accuracy <- sum(diag(a.kmeans.CM))/sum(a.kmeans.CM)

plot(a$x, a$y, col= a.kmeans$cluster, main = "K-means")
points(a.kmeans$centers[, c("x", "y")],
       col=1:3,
       pch = 19,
       cex = 3)

```

Para K-medias se observa un desempeño excelente clasificando de manera casi ideal cada punto alrededor de su centroide. Además la tasa de acierto de su matriz de confusión confirma lo bien que se adapta este algoritmo al modelo, específicamente: 
```{r, echo=F,} 
a.kmeans.accuracy
```


### Partitioning Around Medioids (PAM)
```{r, echo=F}
### Partitioning Around Medioids (PAM)
a.pam <- pam(a[,1:2], 3)
a.pam.CM <- table(a.pam$clustering, a$class)
a.pam.accuracy.CM <- sum(diag(a.pam.CM))/sum(a.pam.CM)

plot(a$x, a$y, col = a.pam$clustering, main = "PAM")
points(a.pam$medoids,
       col=1:3,
       pch = 18,
       cex = 3) 

```

Para PAM se observa que las clases se distribuyen casi idéntico a Kmedias alrededor de sus medioides, a tal punto que su tasa de acierto es idéntica a la de Kmedias. Específicamente: 
```{r, echo=F,} 
a.pam.accuracy.CM 
```


### Clustering Jerárquico
```{r, echo=F}
## Hcluster
a.num <- a # a copy of the dataframe
a.num$class <-NULL # Delete class column
a.num <- as.matrix(a.num) # convert into a matrix

# Calculating each distance method vs each hclust method 
a.hclust = match.hclust(a.num, k = 3, a)

a.dist.mat <- dist(a.num, method = a.hclust[1]) # distance matrix
a.cluster <- hclust(a.dist.mat, method = a.hclust[2]) # apply method
a.ct <- cutree(a.cluster, k =3) # k to generate 3 clusters
a.dendrogram <- as.dendrogram(a.cluster)
plot(a.dendrogram) # dendrogram
rect.hclust(a.cluster, k = 3, border = c("red"))
plot(a$x, a$y, col= a.ct, main = "HCluster")

```

HCluster clusteriza de una manera muy parecida a los dos algoritmo anteriores. Se puede observar en su dendrograma una estructura bastante bien marcada de los tres cluster.
Para Hclust se utilizaron los siguientes métodos.

Métodos usados
```{r, echo=F}
a.hclust[1]
a.hclust[2]

```

Exactitud:
```{r, echo=F}
a.hclust[3]
```




## Archivo *moon.csv*
```{r}
## Exploratory Analysis
moon <- read.csv("moon.csv")
summary(moon)

```

Este es el segundo de cuatro datasets que tiene su última columna con números enteros, los cuales pueden ser utilizados como clase de los grupos.
Se utiliza la función _set class_ previamente creada para establecer los valores de la ultima columa (entre 0 y 1) entre 1 y 2.
El gráfico de los cluster se muestra a continuación.

```{r}
names(moon)[1] <- "x"
names(moon)[2] <- "y"
names(moon)[3] <- "class"
moon <- set_class(moon)
plot(moon$x, moon$y, col=moon$class, main = "Vista Previa")

```


Se eligieron 2 clusters por la forma que presenta el dataset, el cual presenta 2 grupos de datos bien definidos.

Luego se procede al cálculo de K-medias y PAM con 2 centros y medioides respectivamente.


### K-medias
```{r, echo=F}
## K means
moon.kmeans <- k.means2D(dataset = moon, centers = 2)
moon.kmeans.CM <- table(moon.kmeans$cluster, moon$class)
moon.kmeans.accuracy <- sum(diag(moon.kmeans.CM))/sum(moon.kmeans.CM)

plot(moon$x, moon$y, col= moon.kmeans$cluster, main = "K-means")
points(moon.kmeans$centers[, c("x", "y")],
       col=1:2,
       pch = 19,
       cex = 3)

```

Para K-medias se observa un desempeño medianamente bueno clasificando a su manera buena cada punto alrededor de su centroide de forma circular lo que es un comportamiento natural del algoritmo. A pesar de ello la tasa de acierto de su matriz de confusión es medianamente alta. Específicamente: 
```{r, echo=F,} 
moon.kmeans.accuracy

```


### Partitioning Around Medioids (PAM)
```{r}
### Partitioning Around Medioids (PAM)
moon.pam <- pam(moon[,1:2], 2)
moon.pam.CM <- table(moon.pam$clustering, moon$class)
moon.pam.accuracy.CM <- sum(diag(moon.pam.CM))/sum(moon.pam.CM)

plot(moon$x, moon$y, col = moon.pam$clustering, main = "PAM")
points(moon.pam$medoids,
      col=1:2,
      pch = 18,
      cex = 3)

```

Para PAM se observa que las clases se distribuyen muy parecido a Kmedias alrededor de sus medioides. Evidentemente los medioides al pertenecer obligatoriamente al conjunto de datos, su distribución cambia con respecto a los centroides de Kmedias, sin embargo se aprecia que su tasa de acierto es medianamente alta. Específicamente: 
```{r, echo=F,} 
moon.pam.accuracy.CM 
```


### Clustering Jerárquico
```{r, echo=F}
## Hcluster
moon.num <- moon # a copy of the dataframe
moon.num$class <-NULL # Delete class column
moon.num <- as.matrix(moon.num) # convert into a matrix

# Calculating best hclust method 

moon.hclust <- match.hclust(moon.num, 2, moon)

moon.dist.mat <- dist(moon.num, method = moon.hclust[1]) # distance matrix
moon.cluster <- hclust(moon.dist.mat, method = moon.hclust[2]) # apply method
moon.ct <- cutree(moon.cluster, k =2) # k to generate 3 clusters
moon.dendrogram <- as.dendrogram(moon.cluster)
plot(moon.dendrogram, main = "Dendrograma de HClust") # dendrogram
rect.hclust(moon.cluster, k = 2, border = c("red"))
plot(moon$x, moon$y, col= moon.ct, main = "HCluster")

```

En HCluster se puede ver que clusteriza de una manera diferente a la forma que lo haría Kmedias o PAM. Debido a su comportamiento (y a los algoritmos seleccionados para el cálculo de distancia y para la funciń hclust), se puede ver que clusteriza literalmente perfecto cada conjunto; su dendrograma muestra una división utópica de los datos y su tasa de acierto es total.

Métodos usados
```{r, echo=F}
moon.hclust[1]
moon.hclust[2]

```

Exactitud:
```{r, echo=F}
moon.hclust[3]
```



## Archivo *h.csv*
```{r, echo=FALSE}
h.clase = function(numero){
  # Selecting 6 clusters
  if(numero < 7.0)
    return(1)
  else if(numero < 9.0)
    return(2)
  else if(numero < 11.0)
    return(3)
  else if(numero < 12.0)
    return(4)
  else if(numero < 13.0)
    return(5)
  else
    return(6)
}
```


```{r}
## Exploratory Analysis
h <- read.csv("h.csv")
#dim(h)
summary(h)
# c.jambu(h)
```

Se observa que la ultima columna efectivamente es de números flotantes entre 4 y 15, lo cual no sirve para establecer las clases, se procede a realizar una funcion de asignacion de clases, luego a cambiar los nombres de las columnas y finalmente realizar un gráfico con los colores (nuevas clases asignadas con la función de asignación de clases ya pre-establecida) para apreciar mejor la distribucion de los datos

```{r, testgl0, webgl=TRUE}

names(h)[1] <- "x"
names(h)[2] <- "y"
names(h)[3] <- "z"
names(h)[4] <- "class"
for (i in 1:length(h$class)){
  h$class[i] <- h.clase(h$class[i])
}

plot3d(h$x, h$y, h$z, col = h$class, main="Vista Previa")


```


Se eligieron 6 clusters realizando pruebas sucesivas, a medida que el "espiral" se va "desenrollando" da la impresion que tambien se van separando los puntos del conglomerado inicial. Se utilizó ayuda el método "coro de jambú" para ayudar a corroborar que selección de los elementos era cercano a lo esperado.
Los números seleccionados para limitar la regla de asignación de clases se basaron en los valores del primer y tercer cuartil de manera de que la repartición de puntos fuera equitativa.

Luego se procede al cálculo de K-medias y PAM con 5 centros y medioides respectivamente.


### K-medias
```{r, echo=F, testgl1, webgl=TRUE}
## K means
h.kmeans <- k.means3D(dataset = h, centers = 6)
h.kmeans.CM <- table(h.kmeans$cluster, h$class)
h.kmeans.accuracy <- sum(diag(h.kmeans.CM))/sum(h.kmeans.CM)

plot3d(h$x, h$y, h$z, col = h.kmeans$cluster, main = "K-means")
rgl.spheres(h.kmeans$centers[, c("x", "y", "z")], r = 0.4, color = 1:6) 
```
Para K-medias se observa un desempeño bajo teniendo en cuenta que la forma del dataset no es circular (particularmente al ser en 3era dimension esférica o cilíndrica), por lo que tambien su tasa de acierto es bastante baja. Específicamente: 
```{r, echo=F,} 
h.kmeans.accuracy 
```


### Partitioning Around Medioids (PAM)
```{r, echo=F, testgl2, webgl=TRUE}
### Partitioning Around Medioids (PAM)
h.pam <- pam(h[,1:3], 6)
h.pam.CM <- table(h.pam$clustering, h$class)
h.pam.accuracy.CM <- sum(diag(h.pam.CM))/sum(h.pam.CM)

plot3d(h$x, h$y, h$z, col = h.pam$clustering, main = "PAM")
rgl.spheres(h.pam$medoids[, c("x", "y", "z")], r = 0.4, color = 1:6) 
```
Para PAM se observa que muchas clases toman mas espacio que las otras, los medioides se posicionan en la parte interna del esperial, lo que indica que la mayor densidad de elmentos se encuentra en esta sección de la gráfica. También se aprecia que su tasa de acierto es bastante baja. Específicamente: 
```{r, echo=F,} 
h.pam.accuracy.CM 
```


### Clustering Jerárquico
```{r, echo=F, testgl3, webgl=TRUE}
## Hcluster
h.num <- h # a copy of the dataframe
h.num$class <-NULL # Delete class column
h.num <- as.matrix(h.num) # convert into a matrix

# Calculating each distance method vs each hclust method 
h.hclust = match.hclust(h.num, 6, h)

h.dist.mat <- dist(h.num, method = h.hclust[1]) # distance matrix
h.cluster <- hclust(h.dist.mat, method = h.hclust[2]) # apply method
h.ct <- cutree(h.cluster, k =6) # k to generate 3 clusters
h.dendrogram <- as.dendrogram(h.cluster)
plot(h.dendrogram) # dendrogram
rect.hclust(h.cluster, k = 6, border = c("red"))
plot3d(h$x, h$y, h$z, col = h.ct, main = "HCluster")

```
Para HCluster se observa un comportamiento mejor que el ofrecido por PAM o K-Medias, inclusive se puede observar en su dendrograma una estructura bastante limpia. Su tasa de aciertos de acuerdo a su matriz de confusión se muestra a continuación, también se mostrarán los métodos utilizados por Hclust.

Métodos usados
```{r, echo=F}
h.hclust[1]
h.hclust[2]

```

Exactitud:
```{r, echo=F}
h.hclust[3]
```



## Archivo *s.csv*

```{r, echo=FALSE}
s.clase = function(numero){
  # Selecting 2 clusters
  if(numero < 0.0)
    return(1)
  else
    return(2)
} 
```


```{r}
## Exploratory Analysis
s <- read.csv("s.csv")
summary(s)
```

Se observa que la ultima columna efectivamente es de números flotantes entre -4.7 y 4.7 aproximadamente, lo cual no sirve para establecer las clases, se procede a realizar una funcion de asignacion de clases, luego a cambiar los nombres de las columnas y finalmente realizar un gráfico con los colores (nuevas clases asignadas con la función de asignación de clases ya pre-establecida) para apreciar mejor la distribucion de los datos

```{r, testgl4, webgl=TRUE}

names(s)[1] <- "x"
names(s)[2] <- "y"
names(s)[3] <- "z"
names(s)[4] <- "class"
for (i in 1:length(s$class)){
  s$class[i] <- s.clase(s$class[i])
}
plot3d(s$x, s$y, s$z, col = s$class, main = "Vista Previa ")


```


Se eligieron 2 clusters por la forma que presenta el dataset, el cual tuvo que graficarse de cierta forma para poder darle forma similar a una "S". El autor decidió que cada curva de la "S" formara un cluster aparte.
Los números seleccionados para la regla de asignación de clases fueron elegidos acorde a la distribución de los datos de la última columna (antes de que fuera aplicada la regla), su mínimo es -4.7 aproximadamente, su mediana (y coincidencialmente su media) en valores muy cercanos a 0 y su máximo en 4.7.

Luego se procede al cálculo de K-medias y PAM con 5 centros y medioides respectivamente.


### K-medias
```{r, echo=F, testgl5, webgl=TRUE}
## K means
s.kmeans <- k.means3D(dataset = s, centers = 2)
s.kmeans.CM <- table(s.kmeans$cluster, s$class)
s.kmeans.accuracy <- sum(diag(s.kmeans.CM))/sum(s.kmeans.CM)

plot3d(s$x, s$y, s$z, col = s.kmeans$cluster, main = "K-means")
rgl.spheres(s.kmeans$centers[, c("x", "y", "z")], r = 0.1, color = 1:2) 
```
Para K-medias se observa un desempeño altísimo clasificando de manera buena cada punto alrededor de su centroide, lo que coincide con la tasa de acierto de su matriz de confusión que es bastante alta. Específicamente: 
```{r, echo=F,} 
s.kmeans.accuracy 
```


### Partitioning Around Medioids (PAM)
```{r, echo=F, testgl6, webgl=TRUE}
### Partitioning Around Medioids (PAM)
s.pam <- pam(s[,1:3], 2)
s.pam.CM <- table(s.pam$clustering, s$class)
s.pam.accuracy.CM <- sum(diag(s.pam.CM))/sum(s.pam.CM)

plot3d(s$x, s$y, s$z, col = s.pam$clustering, main = "PAM")
rgl.spheres(s.pam$medoids[, c("x", "y", "z")], r = 0.1, color = 1:2) 
```
Para PAM se observa que las clases se distribuyen muy parecido a Kmedias alrededor de sus medioides sin embargo se aprecia que su tasa de acierto es bastante baja. Específicamente: 
```{r, echo=F,} 
s.pam.accuracy.CM 
```


### Clustering Jerárquico
```{r, echo=F, testgl7, webgl=TRUE}
## Hcluster
s.num <- s # a copy of the dataframe
s.num$class <-NULL # Delete class column
s.num <- as.matrix(s.num) # convert into a matrix

# Calculating each distance method vs each hclust method 
s.hclust <- match.hclust(s.num, 2, s)

s.dist.mat <- dist(s.num, method = s.hclust[1]) # distance matrix
s.cluster <- hclust(s.dist.mat, method = s.hclust[2]) # apply method
s.ct <- cutree(s.cluster, k =2) # k to generate 4 clusters
s.dendrogram <- as.dendrogram(s.cluster)
plot(s.dendrogram) # dendrogram
rect.hclust(s.cluster, k = 2, border = c("red"))
plot3d(s$x, s$y, s$z, col = s.ct, main = "HCluster")

```
Para HCluster se observa un comportamiento muy uniforme sin clusterización y mucho peor que la ofrecido por PAM o K-Medias, inclusive se puede observar en su dendrograma una estructura bastante poco entendible. Su tasa de aciertos de acuerdo a su matriz de confusión (que se muestra a continuación, también se mostrarán los métodos utilizados por Hclust) resulta en uno de los valores mas altos de los algoritmos anteriormente probados, sin embargo, su gráfica no muestra tal comportamiento. Muy mal método de clustering en este caso.

Métodos usados
```{r, echo=F}
s.hclust[1]
s.hclust[2]

```

Exactitud:
```{r, echo=F}
s.hclust[3]
```

## Archivo *guess.csv*

Dado este dataset se implementó el método Codo de Jambú para poder determinar el número correcto de clusters a elegir para realizar nuestras pruebas y modelos.
```{r}
## Exploratory Analysis
guess <- read.csv("guess.csv")
c.jambu(guess)
dim(guess)
# names(guess)
# str(guess)
summary(guess)



```

Se observa que no existe la ultima columna con lo cual se debe deescribir el modelo sin clases, en la gráfica se verá la forma del cluster.

```{r}

names(guess)[1] <- "x"
names(guess)[2] <- "y"
plot(guess$x, guess$y) 
```


Se eligieron 3 clusters por la forma del dataset, a medida que el "espiral" se va "desenrollando" da la impresion que tambien se van separando los puntos del conglomerado inicial
Además se eligio un numero entero menor y mas cercano al primer cuartil, mismo para el 3er cuartil

Al no existir columna _clase_ no se pueden crear matrices de confusión para evaluar la exactitud de un modelo.


Al apreciar el gráfico se prueba con 5 centroides.
Luego se procede al cálculo de K-medias y PAM respectivamente 

Ambos algoritmos se corrieron con 5 centros y medioides respectivamente.


```{r, echo=F}
## K means
guess.kmeans <- kmeans(guess[,c("x", "y")], centers = 5)

plot(guess$x, guess$y, col= guess.kmeans$cluster, main = "K-means")
points(guess.kmeans$centers[, c("x", "y")],
       col=1:5,
       pch = 19,
       cex = 3)

## Partitioning Around Medioids (PAM)
guess.pam <- pam(guess[,1:2], 5)

### ***** plot PAM
plot(guess$x, guess$y, col = guess.pam$clustering, main = "PAM")
points(guess.pam$medoids,
       col=1:5,
       pch = 18,
       cex = 3)
```

Luego se realiza el clustering jerárquico. Se utilizaron los métodos single y euclidean que fueron los mejores que lograron dividir el dataset


```{r, echo=F}
## Hcluster
guess.num <- guess # a copy of the dataframe
guess.num <- as.matrix(guess.num) # convert into a matrix
guess.dist.mat <- dist(guess.num, method = dist_methods[1]) # distance matrix
guess.cluster <- hclust(guess.dist.mat, method = hclust_methods[1]) # apply method
guess.ct <- cutree(guess.cluster, k = 5) # to generate k clusters




#### ****** plot HCLUST
dendrogram <- as.dendrogram(guess.cluster)
plot(dendrogram)
rect.hclust(guess.cluster, k = 5, border = c("red"))
corte <- cut(dendrogram, h=5000)$upper # $upper to get useful information instead a forest
plot(corte)
plot(guess$x, guess$y, col= guess.ct, main = "HCluster")
```



## Archivo *help.csv*
```{r, echo=FALSE}
help.clase = function(numero){
  # Selecting 3 clusters
  if(numero < -1.0)
    return(1)
  else if(numero < 7.0)
    return(2)
  else
    return(3)
} 
```


```{r}
## Exploratory Analysis
help <- read.csv("help.csv")
summary(help)
```

Se observa que la ultima columna efectivamente es de números flotantes entre -4.7 y 14 aproximadamente, lo cual no sirve para establecer las clases, se procede a aplicar una función de asignacion de clases, luego a cambiar los nombres de las columnas y finalmente realizar un gráfico con los colores (nuevas clases asignadas con la función de asignación de clases ya pre-establecida) para apreciar mejor la distribucion de los datos

```{r, testgl8, webgl=TRUE}
names(help)[1] <- "x"
names(help)[2] <- "y"
names(help)[3] <- "z"
names(help)[4] <- "class"
for (i in 1:length(help$class)){
  help$class[i] <- help.clase(help$class[i])
}
plot3d(help$x, help$y, help$z, col = help$class, main = "Vista Previa")

```


Se eligieron 3 clusters por la forma que presenta el dataset, el cual tuvo que graficarse de cierta forma para poder darle forma similar a dos "S" y una especie de espiral, similares a los datasets _h.csv_ y _s.csv_. Se decidió que cada _"letra"_ formara un cluster aparte.
Los números seleccionados para la regla de asignación de clases fueron elegidos acorde a la distribución de los datos de la última columna (antes de que fuera aplicada la regla), su cuartil 1 y su cuartil 3 especificamente para que fuera separada equitativamente

Luego se procede al cálculo de K-medias y PAM con 3 centros y medioides respectivamente.


### K-medias
```{r, echo=F, testgl9, webgl=TRUE}
## K means
help.kmeans <- k.means3D(dataset = help, centers = 3)
help.kmeans.CM <- table(help.kmeans$cluster, help$class)
help.kmeans.accuracy <- sum(diag(help.kmeans.CM))/sum(help.kmeans.CM)

plot3d(help$x, help$y, help$z, col = help.kmeans$cluster, main = "K-means")
rgl.spheres(help.kmeans$centers[, c("x", "y", "z")], r = 0.8, color = 1:3) 
```
Para K-medias se observa un desempeño altísimo clasificando de manera buena cada punto alrededor de su centroide, y así a su vez de cada letra. A pesar de ello la tasa de acierto de su matriz de confusión no es tan alta. Específicamente: 
```{r, echo=F,} 
help.kmeans.accuracy
```


### Partitioning Around Medioids (PAM)
```{r, echo=F, testgl10, webgl=TRUE}
### Partitioning Around Medioids (PAM)
help.pam <- pam(help[,1:3], 3)
help.pam.CM <- table(help.pam$clustering, help$class)
help.pam.accuracy.CM <- sum(diag(help.pam.CM))/sum(help.pam.CM)

plot3d(help$x, help$y, help$z, col = help.pam$clustering, main = "PAM")
rgl.spheres(help.pam$medoids[, c("x", "y", "z")], r = 0.8, color = 1:3) 
```
Para PAM se observa que las clases se distribuyen muy parecido a Kmedias alrededor de sus medioides sin embargo se aprecia que su tasa de acierto es bastante baja. Específicamente: 
```{r, echo=F,} 
help.pam.accuracy.CM 
```


### Clustering Jerárquico
```{r, echo=F}
## Hcluster
help.num <- help # a copy of the dataframe
help.num$class <-NULL # Delete class column
help.num <- as.matrix(help.num) # convert into a matrix

# Calculating each distance method vs each hclust method 
help.hclust = match.hclust(help.num, 3, help)

help.dist.mat <- dist(help.num, method = help.hclust[1]) # distance matrix
help.cluster <- hclust(help.dist.mat, method = help.hclust[2]) # apply method
help.ct <- cutree(help.cluster, k =3) # k to generate 3 clusters
help.dendrogram <- as.dendrogram(help.cluster)
plot(help.dendrogram) # dendrogram
rect.hclust(help.cluster, k = 3, border = c("red"))
```
```{r, echo=F, testgl11, webgl=TRUE}
plot3d(help$x, help$y, help$z, col = help.ct, main = "HCluster")

```
Para HCluster se puede ver que clusteriza de una manera diferente a la forma que lo haría Kmedias o PAM, de forma que, si asumimos que cada cluster es una letra ("S", "O" y "S"), no toma cada letra como un cluster sino que toma un par de "letras" como un solo cluster mientras que divide el otro en dos clusters, se puede observar en su dendrograma una estructura bastante similar teniendo en cuenta que el grupo grande es el de las dos "letras" y los pequeños que se dividen en 2, son cada una de las "letras" que fueron separadas en un cluster cada una. Su tasa de aciertos de acuerdo a su matriz de confusión (que se muestra a continuación, también se mostrarán los métodos utilizados por Hclust) resulta en uno de los valores mas altos de los algoritmos anteriormente probados.

Métodos usados
```{r, echo=F}
help.hclust[1]
help.hclust[2]

```

Exactitud:
```{r, echo=F}
help.hclust[3]
```


























































