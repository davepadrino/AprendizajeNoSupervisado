# Getting final format for Google API
api_url = paste(c(base, paste0(c(origin, destination, key, mode, language), collapse = "&")), collapse = "")
return(api_url)
}
get_data = function(api_url){
return(fromJSON(api_url))
}
# To Complete
parse_data = function(json){
}
?confusion.matrix
?confusionMatrix
setwd("~/Desktop/univ/Minería de Datos/Tareas/Tarea_3/AprendizajeNoSupervisado")
library("stringr")
library("caret")
library("clue")
library("cluster")
library("rgl")
# First set the location directory
hclust_methods <- c("ward.D", "single", "complete", "average", "mcquitty", "median", "centroid", "ward.D2")
dist_methods <- c("euclidean", "maximum", "manhattan", "minkowski")
############################ functions #######################################
# codo de jambu
c.jambu= function(d){
mydata <- d
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(mydata, centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
}
# Calculating best kmeans algorithm 2D
k.means2D = function(dataset, centers){
kmeans.accuracy.CM <- 0
kmeans.better.accuracy.CM <- 0
for (i in 1:10){
kmeans <- kmeans(dataset[,c("x", "y")], centers = centers)
kmeans.CM <- table(kmeans$cluster, dataset$class)
kmeans.accuracy.CM <- sum(diag(kmeans.CM))/sum(kmeans.CM)
if (kmeans.accuracy.CM > kmeans.better.accuracy.CM){
kmeans.better <- kmeans
kmeans.better.accuracy.CM <- kmeans.accuracy.CM # useless (?)
}
}
return(kmeans.better)
}
# Calculating best kmeans algorithm 2D
k.means3D = function(dataset, centers){
kmeans.accuracy.CM <- 0
kmeans.better.accuracy.CM <- 0
for (i in 1:10){
kmeans <- kmeans(dataset[,c("x", "y","z")], centers = centers)
kmeans.CM <- table(kmeans$cluster, dataset$class)
kmeans.accuracy.CM <- sum(diag(kmeans.CM))/sum(kmeans.CM)
if (kmeans.accuracy.CM > kmeans.better.accuracy.CM){
kmeans.better <- kmeans
kmeans.better.accuracy.CM <- kmeans.accuracy.CM # useless (?)
}
}
return(kmeans.better)
}
# Comparing kmeans vs PAM
compare.kmeans.pam = function(kmeans.acc, pam.acc){
kmeans.vs.pam <- 0
if (kmeans.acc >= pam.acc){
#a.kmeans.vs.pam <- kmeans.acc
kmeans.vs.pam <- c("kmeans", kmeans.acc)
return(kmeans.vs.pam)
}else{
#a.kmeans.vs.pam <- pam.acc
kmeans.vs.pam <- c("pam", pam.acc)
return(kmeans.vs.pam)
}
}
# Calculating each distance method vs each hclust method
match.hclust = function(matrix, k, dataset){
better.accuracy <- 0
for (i in 1:length(dist_methods)){
for (j in 1:length(hclust_methods)){
dist.mat <- dist(matrix, method = dist_methods[i]) # distance matrix
cluster <- hclust(dist.mat, method = hclust_methods[j]) # apply method
ct <- cutree(cluster, k) # k to generate k clusters
CM <- table(as.factor(dataset$class), as.factor(ct))
accuracy.CM <- sum(diag(CM))/sum(CM)
if (accuracy.CM > better.accuracy){
better.accuracy <- accuracy.CM
better <- c(dist_methods[i], hclust_methods[j], accuracy.CM)
}
}
}
return(better)
}
title: "tarea3"
a <- read.csv("a.csv")
plot(a[1], a[2])
h <- read.csv("h.csv")
names(h)[1] <- "x"
names(h)[2] <- "y"
names(h)[3] <- "z"
names(h)[4] <- "class"
for (i in 1:length(h$class)){
h$class[i] <- h.clase(h$class[i])
}
setwd("~/Desktop/univ/Minería de Datos/Tareas/Tarea_3/AprendizajeNoSupervisado")
h <- read.csv("h.csv")
names(h)[1] <- "x"
names(h)[2] <- "y"
names(h)[3] <- "z"
names(h)[4] <- "class"
for (i in 1:length(h$class)){
h$class[i] <- h.clase(h$class[i])
}
h.clase = function(numero){
# Selecting 3 clusters
if(numero < 5.0)
return(1)
else if(numero < 7.0)
return(2)
else if(numero < 9.0)
return(3)
else if(numero < 11.0)
return(4)
else
return(5)
}
for (i in 1:length(h$class)){
h$class[i] <- h.clase(h$class[i])
}
plot3d(h$x, h$y, h$z, col = h$class)
summary(h)
h <- read.csv("h.csv")
summary(h)
c.jambu(h)
h.clase = function(numero){
# Selecting 3 clusters
if(numero < 6.0)
return(1)
else if(numero < 8.0)
return(2)
else if(numero < 10.0)
return(3)
else if(numero < 12.0)
return(4)
else
return(5)
}
names(h)[1] <- "x"
names(h)[2] <- "y"
names(h)[3] <- "z"
names(h)[4] <- "class"
for (i in 1:length(h$class)){
h$class[i] <- h.clase(h$class[i])
}
summary(h)
plot3d(h$x, h$y, h$z, col = h$class)
library("knitr")
library("rglwidget")
install("rglwidget")
install.packages("rglwidget")
s <- read.csv("s.csv")
c.jambu(s)
h.kmeans <- k.means3D(dataset = h, centers = 5)
h.kmeans.CM <- table(h.kmeans$cluster, h$class)
h.kmeans.accuracy <- sum(diag(h.kmeans.CM))/sum(h.kmeans.CM)
h.pam.CM <- table(h.pam$clustering, h$class)
h.pam.accuracy.CM <- sum(diag(h.pam.CM))/sum(h.pam.CM)
h.pam <- pam(h[,1:3], 5)
h.pam.CM <- table(h.pam$clustering, h$class)
h.pam.accuracy.CM <- sum(diag(h.pam.CM))/sum(h.pam.CM)
h <- read.csv("h.csv")
names(h)[1] <- "x"
names(h)[2] <- "y"
names(h)[3] <- "z"
names(h)[4] <- "class"
for (i in 1:length(h$class)){
h$class[i] <- h.clase(h$class[i])
}
#dim(h)
#names(h)
#str(h)
summary(h)
# c.jambu(h)
h.kmeans <- k.means3D(dataset = h, centers = 5)
h.kmeans.CM <- table(h.kmeans$cluster, h$class)
h.kmeans.accuracy <- sum(diag(h.kmeans.CM))/sum(h.kmeans.CM)
h.pam <- pam(h[,1:3], 5)
h.pam.CM <- table(h.pam$clustering, h$class)
h.pam.accuracy.CM <- sum(diag(h.pam.CM))/sum(h.pam.CM)
h.kmeans.vs.pam <- compare.kmeans.pam(h.kmeans.accuracy, h.pam.accuracy.CM)
h.num <- h # a copy of the dataframe
h.num$class <-NULL # Delete class column
h.num <- as.matrix(h.num) # convert into a matrix
# Calculating each distance method vs each hclust method
h.hclust = match.hclust(h.num, 5, h)
h.hclust.better.accuracy <- 0
for (i in 1:length(dist_methods)){
for (j in 1:length(hclust_methods)){
h.dist.mat <- dist(h.num, method = dist_methods[i]) # distance matrix
h.cluster <- hclust(h.dist.mat, method = hclust_methods[j]) # apply method
h.ct <- cutree(h.cluster, k =5) # k to generate 3 clusters
h.hclust.CM <- table(as.factor(h$class), as.factor(h.ct))
h.hclust.accuracy.CM <- sum(diag(h.hclust.CM))/sum(h.hclust.CM)
if (h.hclust.accuracy.CM > h.hclust.better.accuracy){
h.better <- c(dist_methods[i], hclust_methods[j])
h.cluster.better <- h.cluster ##
h.ct.better <- h.ct ##
h.hclust.better.accuracy <- h.hclust.accuracy.CM
}
}
}
h.dendrogram <- as.dendrogram(h.cluster.better)
plot(h.dendrogram) # dendrogram
rect.hclust(h.cluster.better, k = 5, border = c("red"))
corte <- cut(h.dendrogram, h=205)$upper # $upper to get useful information instead a forest
plot(corte)
c.jambu(s)
s.clase = function(numero){
# Selecting 3 clusters
if(numero < -1.0)
return(1)
else if(numero < 2.0)
return(2)
else
return(3)
}
s <- read.csv("s.csv")
names(s)[1] <- "x"
names(s)[2] <- "y"
names(s)[3] <- "z"
names(s)[4] <- "class"
plot3d(s$x, s$y, s$z)
for (i in 1:length(s$class)){
s$class[i] <- s.clase(s$class[i])
}
plot3d(s$x, s$y, s$z, col = rainbow(s$class))
s.kmeans <- k.means3D(dataset = s, centers = 3)
s.kmeans.CM <- table(s.kmeans$cluster, s$class)
s.kmeans.accuracy <- sum(diag(s.kmeans.CM))/sum(s.kmeans.CM)
rgl.open()
rgl.bg(color = "white") # Setup the background color
plot3d(s$x, s$y, s$z, col = rainbow(s.kmeans$cluster), main = "K-means")
rgl.spheres(s.kmeans$centers[, c("x", "y", "z")], r = 0.1, color = 1:3)
rgl.open()
rgl.bg(color = "white") # Setup the background color
plot3d(s$x, s$y, s$z, col = s.kmeans$cluster, main = "K-means")
rgl.spheres(s.kmeans$centers[, c("x", "y", "z")], r = 0.1, color = 1:3)
s <- read.csv("s.csv")
summary(s)
s.clase = function(numero){
# Selecting 3 clusters
if(numero < -2.0)
return(1)
else if(numero < 0.0)
return(2)
else
return(3)
}
names(s)[1] <- "x"
names(s)[2] <- "y"
names(s)[3] <- "z"
names(s)[4] <- "class"
for (i in 1:length(s$class)){
s$class[i] <- s.clase(s$class[i])
}
plot3d(s$x, s$y, s$z, col = s$class)
s <- read.csv("s.csv")
summary(s)
str(s)
guess <- read.csv("guess.csv")
names(guess)[1] <- "x"
names(guess)[2] <- "y"
guess.num <- guess # a copy of the dataframe
guess.num <- as.matrix(guess.num) # convert into a matrix
guess.dist.mat <- dist(guess.num, method = dist_methods[1]) # distance matrix
guess.cluster <- hclust(guess.dist.mat, method = hclust_methods[1]) # apply method
guess.ct <- cutree(guess.cluster, k = 5) # to generate k clusters
dendrogram <- as.dendrogram(guess.cluster)
plot(dendrogram)
rect.hclust(guess.cluster, k = 5, border = c("red"))
corte <- cut(dendrogram, h=500000)$upper # $upper to get useful information instead a forest
plot(corte)
corte <- cut(dendrogram, h=50000)$upper # $upper to get useful information instead a forest
plot(corte)
plot(dendrogram)
rect.hclust(guess.cluster, k = 5, border = c("red"))
corte <- cut(dendrogram, h=5000)$upper # $upper to get useful information instead a forest
plot(corte)
plot(guess$x, guess$y, col= guess.ct, main = "HCluster")
help.clase = function(numero){
# Selecting 3 clusters
if(numero < -1.0)
return(1)
else if(numero < 2.0)
return(2)
else
return(3)
}
help <- read.csv("help.csv")
summary(help)
names(help)[1] <- "x"
names(help)[2] <- "y"
names(help)[3] <- "z"
names(help)[4] <- "class"
for (i in 1:length(help$class)){
help$class[i] <- help.clase(help$class[i])
}
summary(help)
plot3d(help$x, help$y, help$z)
plot3d(help$x, help$y, help$z, col = 1:3)
help.clase = function(numero){
# Selecting 3 clusters
if(numero < -1.0)
return(1)
else if(numero < 2.0)
return(2)
else
return(3)
}
help <- read.csv("help.csv")
c.jambu(help)
a <- read.csv("a.csv")
summary(a$X1.000000)
summary(a[3])
a[3][1]
library("stringr")
library("caret")
library("clue")
library("cluster")
library("rgl")
# First set the location directory
hclust_methods <- c("ward.D", "single", "complete", "average", "mcquitty", "median", "centroid", "ward.D2")
dist_methods <- c("euclidean", "maximum", "manhattan", "minkowski")
############################ functions #######################################
#Set classes after 1
set_class = function(dataset){
dataset$class <-as.numeric(dataset$class)
dataset$class <- dataset$class + 1
}
# codo de jambu
c.jambu= function(d){
mydata <- d
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(mydata, centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
}
# Calculating best kmeans algorithm 2D
k.means2D = function(dataset, centers){
kmeans.accuracy.CM <- 0
kmeans.better.accuracy.CM <- 0
for (i in 1:10){
kmeans <- kmeans(dataset[,c("x", "y")], centers = centers)
kmeans.CM <- table(kmeans$cluster, dataset$class)
kmeans.accuracy.CM <- sum(diag(kmeans.CM))/sum(kmeans.CM)
if (kmeans.accuracy.CM > kmeans.better.accuracy.CM){
kmeans.better <- kmeans
kmeans.better.accuracy.CM <- kmeans.accuracy.CM # useless (?)
}
}
return(kmeans.better)
}
# Calculating best kmeans algorithm 2D
k.means3D = function(dataset, centers){
kmeans.accuracy.CM <- 0
kmeans.better.accuracy.CM <- 0
for (i in 1:10){
kmeans <- kmeans(dataset[,c("x", "y","z")], centers = centers)
kmeans.CM <- table(kmeans$cluster, dataset$class)
kmeans.accuracy.CM <- sum(diag(kmeans.CM))/sum(kmeans.CM)
if (kmeans.accuracy.CM > kmeans.better.accuracy.CM){
kmeans.better <- kmeans
kmeans.better.accuracy.CM <- kmeans.accuracy.CM # useless (?)
}
}
return(kmeans.better)
}
# Comparing kmeans vs PAM
compare.kmeans.pam = function(kmeans.acc, pam.acc){
kmeans.vs.pam <- 0
if (kmeans.acc >= pam.acc){
#a.kmeans.vs.pam <- kmeans.acc
kmeans.vs.pam <- c("kmeans", kmeans.acc)
return(kmeans.vs.pam)
}else{
#a.kmeans.vs.pam <- pam.acc
kmeans.vs.pam <- c("pam", pam.acc)
return(kmeans.vs.pam)
}
}
# Calculating each distance method vs each hclust method
match.hclust = function(matrix, k, dataset){
better.accuracy <- 0
for (i in 1:length(dist_methods)){
for (j in 1:length(hclust_methods)){
dist.mat <- dist(matrix, method = dist_methods[i]) # distance matrix
cluster <- hclust(dist.mat, method = hclust_methods[j]) # apply method
ct <- cutree(cluster, k) # k to generate k clusters
CM <- table(as.factor(dataset$class), as.factor(ct))
accuracy.CM <- sum(diag(CM))/sum(CM)
if (accuracy.CM > better.accuracy){
better.accuracy <- accuracy.CM
better <- c(dist_methods[i], hclust_methods[j], accuracy.CM)
}
}
}
return(better)
}
a <- read.csv("a.csv")
set_class(a)
names(a)[1] <- "x"
names(a)[2] <- "y"
names(a)[3] <- "class"
set_class(a)
summary(a)
summary(a)
set_class(a)
summary(a)
set_class = function(dataset){
dataset$class <-as.numeric(dataset$class)
dataset$class <- dataset$class + 1
return(dataset)
}
a <- read.csv("a.csv")
set_class(a)
names(a)[1] <- "x"
names(a)[2] <- "y"
names(a)[3] <- "class"
set_class(a)
a <- set_class(a)
summary(a)
h.clase = function(numero){
# Selecting 5 clusters
if(numero < 6.0)
return(1)
else if(numero < 8.0)
return(2)
else if(numero < 10.0)
return(3)
else if(numero < 12.0)
return(4)
else
return(5)
}
h <- read.csv("h.csv")
names(h)[1] <- "x"
names(h)[2] <- "y"
names(h)[3] <- "z"
names(h)[4] <- "class"
for (i in 1:length(h$class)){
h$class[i] <- h.clase(h$class[i])
}
help.clase = function(numero){
# Selecting 3 clusters
if(numero < -1.0)
return(1)
else if(numero < 2.0)
return(2)
else
return(3)
}
help <- read.csv("help.csv")
names(help)[1] <- "x"
names(help)[2] <- "y"
names(help)[3] <- "z"
names(help)[4] <- "class"
for (i in 1:length(help$class)){
help$class[i] <- help.clase(help$class[i])
}
c.jambu(help)
plot3d(help$x, help$y, help$z)
help <- read.csv("help.csv")
names(help)[1] <- "x"
names(help)[2] <- "y"
names(help)[3] <- "z"
names(help)[4] <- "class"
for (i in 1:length(help$class)){
help$class[i] <- help.clase(help$class[i])
}
help.kmeans <- k.means3D(dataset = help, centers = 3)
help.kmeans.CM <- table(help.kmeans$cluster, help$class)
help.kmeans.accuracy <- sum(diag(help.kmeans.CM))/sum(help.kmeans.CM)
rgl.open()
rgl.bg(color = "white") # Setup the background color
plot3d(help$x, help$y, help$z, col = help.kmeans$cluster, main = "K-means")
rgl.spheres(help.kmeans$centers[, c("x", "y", "z")], r = 0.4, color = 1:5)
help.pam <- pam(help[,1:3], 3)
help.pam.CM <- table(help.pam$clustering, help$class)
help.pam.accuracy.CM <- sum(diag(help.pam.CM))/sum(help.pam.CM)
plot3d(help$x, help$y, help$z, col = help.pam$clustering, main = "PAM")
rgl.spheres(help.pam$medoids[, c("x", "y", "z")], r = 0.2, color = 1:3)
help.pam.accuracy.CM
help.num <- help # a copy of the dataframe
help.num$class <-NULL # Delete class column
help.num <- as.matrix(help.num) # convert into a matrix
help.hclust = match.hclust(help.num, 3, help)
help.dist.mat <- dist(help.num, method = help.hclust[1]) # distance matrix
help.cluster <- hclust(help.dist.mat, method = help.hclust[2]) # apply method
help.ct <- cutree(help.cluster, k =3) # k to generate 3 clusters
help.dendrogram <- as.dendrogram(help.cluster)
plot(help.dendrogram) # dendrogram
rect.hclust(help.cluster, k = 3, border = c("red"))
corte <- cut(help.dendrogram, h=18)$upper # $upper to get useful information instead a forest
plot(corte)
corte <- cut(help.dendrogram, h=19)$upper # $upper to get useful information instead a forest
plot(corte)
plot(help.dendrogram) # dendrogram
rect.hclust(help.cluster, k = 3, border = c("red"))
corte <- cut(help.dendrogram, h=18,5)$upper # $upper to get useful information instead a forest
plot(corte)
corte <- cut(help.dendrogram, h=18.5)$upper # $upper to get useful information instead a forest
plot(corte)
rgl.open()
rgl.bg(color = "white") # Setup the background color
plot3d(help$x, help$y, help$z, col = help.ct, main = "HCluster")
help.hclust[3]
