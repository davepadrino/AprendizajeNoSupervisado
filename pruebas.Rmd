---
title: "Tarea 3"
author: "David Padrino"
date: "Domingo, 10 de abril de 2016"
widescreen: yes
runtime: shiny
output: html_document
---
# Introducción
Primero que nada se debe establecer el _location directory_ como la bicación actual del archivo.
Se instalan e invocan las bibliotecas necesarias, se leen los archivos a medida que se va trabajando con ellos
```{r}  
# install.packages("clue")
# install.packages("Rcmdr")
# install.packages("rglwidget")
library("stringr")
library("caret")
library("clue")
library("cluster")
library("rgl")
library("knitr")
library("rglwidget")
knit_hooks$set(webgl = hook_webgl)
```

Se procede a crear 2 arreglos con los metodos necesarios para realizar el clustering jerárquico.

```{r}
hclust_methods <- c("ward.D", "single", "complete", "average", "mcquitty", "median", "centroid", "ward.D2")
dist_methods <- c("euclidean", "maximum", "manhattan", "minkowski")
```

Se crearon ciertas funciones por comodidad, como la implementación del codo de jambú, calculo de kmedias para 2D y 3D, comparación de Kmedias y PAM a través de sus matries de confusión, etc.
```{r, echo=FALSE}
#Set classes after 1
set_class = function(dataset){
  dataset$class <-as.numeric(dataset$class)
  dataset$class <- dataset$class + 1
  return(dataset)
}


# codo de jambu 
c.jambu= function(d){
  mydata <- d
  wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
  for (i in 2:15) wss[i] <- sum(kmeans(mydata, centers=i)$withinss)
  plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
}

# Calculating best kmeans algorithm 2D
k.means2D = function(dataset, centers){
  kmeans.accuracy.CM <- 0
  kmeans.better.accuracy.CM <- 0
  for (i in 1:10){
    kmeans <- kmeans(dataset[,c("x", "y")], centers = centers)
    kmeans.CM <- table(kmeans$cluster, dataset$class)
    kmeans.accuracy.CM <- sum(diag(kmeans.CM))/sum(kmeans.CM)
    if (kmeans.accuracy.CM > kmeans.better.accuracy.CM){
      kmeans.better <- kmeans
      kmeans.better.accuracy.CM <- kmeans.accuracy.CM # useless (?)
    }
  }
  return(kmeans.better)
}


# Calculating best kmeans algorithm 2D
k.means3D = function(dataset, centers){
  kmeans.accuracy.CM <- 0
  kmeans.better.accuracy.CM <- 0
  for (i in 1:10){
    kmeans <- kmeans(dataset[,c("x", "y","z")], centers = centers)
    kmeans.CM <- table(kmeans$cluster, dataset$class)
    kmeans.accuracy.CM <- sum(diag(kmeans.CM))/sum(kmeans.CM)
    if (kmeans.accuracy.CM > kmeans.better.accuracy.CM){
      kmeans.better <- kmeans
      kmeans.better.accuracy.CM <- kmeans.accuracy.CM # useless (?)
    }
  }
  return(kmeans.better)
}


# Comparing kmeans vs PAM
compare.kmeans.pam = function(kmeans.acc, pam.acc){
  kmeans.vs.pam <- 0
  if (kmeans.acc >= pam.acc){
    #a.kmeans.vs.pam <- kmeans.acc
    kmeans.vs.pam <- c("kmeans", kmeans.acc)
    return(kmeans.vs.pam) 
  }else{
    #a.kmeans.vs.pam <- pam.acc
    kmeans.vs.pam <- c("pam", pam.acc)
    return(kmeans.vs.pam) 
  }
}

# Calculating each distance method vs each hclust method 
match.hclust = function(matrix, k, dataset){
  better.accuracy <- 0
  for (i in 1:length(dist_methods)){
    for (j in 1:length(hclust_methods)){
      dist.mat <- dist(matrix, method = dist_methods[i]) # distance matrix
      cluster <- hclust(dist.mat, method = hclust_methods[j]) # apply method
      ct <- cutree(cluster, k) # k to generate k clusters
      CM <- table(as.factor(dataset$class), as.factor(ct))
      accuracy.CM <- sum(diag(CM))/sum(CM)
      if (accuracy.CM > better.accuracy){
        better.accuracy <- accuracy.CM
        better <- c(dist_methods[i], hclust_methods[j], accuracy.CM)
      }
    }
  }
  return(better)
}
```

## Archivo *guess.csv*

```{r}
help.clase = function(numero){
  # Selecting 3 clusters
  if(numero < -1.0)
    return(1)
  else if(numero < 2.0)
    return(2)
  else
    return(3)
} 

```

```{r}
## Exploratory Analysis
help <- read.csv("help.csv")
dim(help)
#names(help)
#str(help)
summary(help)

```

La ultima columna fue convertida en numero entero para darle clase al dataset.

### Se ven 3 clusters en forma de especie de letras. Cada "letra" define un cluster diferente
```{r, testgl4, webgl=TRUE}
names(help)[1] <- "x"
names(help)[2] <- "y"
names(help)[3] <- "z"
names(help)[4] <- "class"
for (i in 1:length(help$class)){
  help$class[i] <- help.clase(help$class[i])
}
plot3d(help$x, help$y, help$z)
```

### Al aplicar la regla de asignación de clases se divide la última columna en numeros enteros que al final sirven para darles una clase a los cluster. Pero gráficamente estos estan desordenados y no se ven los cluster identificados
```{r, testgl5, webgl=TRUE}
summary(help)
plot3d(help$x, help$y, help$z, col = 1:3)
```

### La solución ideal para asignar de manera correcta los cluster es a través de algoritmos de clusterización. En los siguientes fragmentos de código se analizará el mejor de los casos.

Se eligieron 3 clusters por la forma del dataset.

Dadas ciertas condiciones _pre-probadas_ se procederá a mostrar el comportamiento de este dataset con los 3 métodos, Kmedias, PAM y Hclust

### Para K-Medias

```{r, echo=F, testgl7, webgl=TRUE}
## K means
help.kmeans <- k.means3D(dataset = help, centers = 3)
help.kmeans.CM <- table(help.kmeans$cluster, help$class)
help.kmeans.accuracy <- sum(diag(help.kmeans.CM))/sum(help.kmeans.CM)

plot3d(help$x, help$y, help$z, col = help.kmeans$cluster, main = "K-means")
rgl.spheres(help.kmeans$centers[, c("x", "y", "z")], r = 0.4, color = 1:3) 
```

Podemos ver que distribuye excelente cada una de las 3 "letras" con cada color, lo cual al parecer es una buena representación, sin emabrgo, cuando se evaluúa la calidad del modelo a través de una matriz de confusión el resultado es el siguiente:
```{r, echo=F}
help.kmeans.accuracy
```

### Para PAM
```{r, echo=F, testgl8, webgl=TRUE}
## Partitioning Around Medioids (PAM)
help.pam <- pam(help[,1:3], 3)
help.pam.CM <- table(help.pam$clustering, help$class)
help.pam.accuracy.CM <- sum(diag(help.pam.CM))/sum(help.pam.CM)

plot3d(help$x, help$y, help$z, col = help.pam$clustering, main = "PAM")
rgl.spheres(help.pam$medoids[, c("x", "y", "z")], r = 0.2, color = 1:3) 
```

Tambien se observa que elige cada "letra" como parte de un cluster por separado, su calidad evaluada a través de su matriz de confusión es la siguiente.
```{r, echo=F}
help.pam.accuracy.CM
```


### Para Hclust
```{r, echo=F, testgl9, webgl=TRUE}
## Hcluster
help.num <- help # a copy of the dataframe
help.num$class <-NULL # Delete class column
help.num <- as.matrix(help.num) # convert into a matrix
help.hclust = match.hclust(help.num, 3, help)
help.dist.mat <- dist(help.num, method = help.hclust[1]) # distance matrix
help.cluster <- hclust(help.dist.mat, method = help.hclust[2]) # apply method
help.ct <- cutree(help.cluster, k =3) # k to generate 3 clusters
help.dendrogram <- as.dendrogram(help.cluster)
plot(help.dendrogram) # dendrogram
rect.hclust(help.cluster, k = 3, border = c("red"))
corte <- cut(help.dendrogram, h=18.5)$upper # $upper to get useful information instead a forest
plot(corte)
rgl.open()
rgl.bg(color = "white") # Setup the background color
plot3d(help$x, help$y, help$z, col = help.ct, main = "HCluster")

```


Con una exactitud de:
```{r,  echo=F}
help.hclust[3]
```


Como s epuede ver, no siempre los cluster _mejor divididos_ son hechos con una buena calidad a través de una matriz de confusión.
































































































