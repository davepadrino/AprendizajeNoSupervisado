---
title: "Tarea 3"
author: "David Padrino"
date: "Domingo, 10 de abril de 2016"
widescreen: yes
runtime: shiny
output: html_document
---
# Introducción
Primero que nada se debe establecer el _location directory_ como la ubicación actual del archivo.
Se instalan e invocan las bibliotecas necesarias, se leen los archivos a medida que se va trabajando con ellos


# Puesta a punto
## 1 - Se cargan las bibliotecas necesarias
```{r setup, warning=FALSE, message=FALSE}  
# install.packages("clue")
# install.packages("Rcmdr")
# install.packages("rglwidget")
library("stringr")
library("caret")
library("clue")
library("cluster")
library("rgl")
library("knitr")
library("rglwidget")
knit_hooks$set(webgl = hook_webgl)
```

## 2 - Se procede a crear 2 arreglos con los metodos necesarios para realizar el clustering jerárquico. 
Se aclara que **para todos los metodos donde se usó clustering jerárquico se utilizó una función personalizada que selecciona la mejor combinación de los métodos tanto de distancia como de clustering expuestos en los arreglos debajo de este párrafo, basado en la tasa de aciertos de su matriz de confusión**




```{r}
hclust_methods <- c("ward.D", "single", "complete", "average", "mcquitty", "median", "centroid", "ward.D2")
dist_methods <- c("euclidean", "maximum", "manhattan", "minkowski")
```

## 3 - Se crearon funciones personalizadas
Algunas de ellas son la implementación del codo de jambú, calculo de kmedias para 2D y 3D, comparación de Kmedias y PAM a través de sus matries de confusión, etc.
```{r, echo=FALSE}
#Set classes after 1
set_class = function(dataset){
  dataset$class <-as.numeric(dataset$class)
  dataset$class <- dataset$class + 1
  return(dataset)
}

# codo de jambu 
c.jambu= function(d){
  mydata <- d
  wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
  for (i in 2:15) wss[i] <- sum(kmeans(mydata, centers=i)$withinss)
  plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
}

# Calculating best kmeans algorithm 2D
k.means2D = function(dataset, centers){
  kmeans.accuracy.CM <- 0
  kmeans.better.accuracy.CM <- 0
  for (i in 1:10){
    kmeans <- kmeans(dataset[,c("x", "y")], centers = centers)
    kmeans.CM <- table(kmeans$cluster, dataset$class)
    kmeans.accuracy.CM <- sum(diag(kmeans.CM))/sum(kmeans.CM)
    if (kmeans.accuracy.CM > kmeans.better.accuracy.CM){
      kmeans.better <- kmeans
      kmeans.better.accuracy.CM <- kmeans.accuracy.CM # useless (?)
    }
  }
  return(kmeans.better)
}


# Calculating best kmeans algorithm 2D
k.means3D = function(dataset, centers){
  kmeans.accuracy.CM <- 0
  kmeans.better.accuracy.CM <- 0
  for (i in 1:10){
    kmeans <- kmeans(dataset[,c("x", "y","z")], centers = centers)
    kmeans.CM <- table(kmeans$cluster, dataset$class)
    kmeans.accuracy.CM <- sum(diag(kmeans.CM))/sum(kmeans.CM)
    if (kmeans.accuracy.CM > kmeans.better.accuracy.CM){
      kmeans.better <- kmeans
      kmeans.better.accuracy.CM <- kmeans.accuracy.CM # useless (?)
    }
  }
  return(kmeans.better)
}


# Comparing kmeans vs PAM
compare.kmeans.pam = function(kmeans.acc, pam.acc){
  kmeans.vs.pam <- 0
  if (kmeans.acc >= pam.acc){
    #a.kmeans.vs.pam <- kmeans.acc
    kmeans.vs.pam <- c("kmeans", kmeans.acc)
    return(kmeans.vs.pam) 
  }else{
    #a.kmeans.vs.pam <- pam.acc
    kmeans.vs.pam <- c("pam", pam.acc)
    return(kmeans.vs.pam) 
  }
}

# Calculating each distance method vs each hclust method 
match.hclust = function(matrix, k, dataset){
  better.accuracy <- 0
  for (i in 1:length(dist_methods)){
    for (j in 1:length(hclust_methods)){
      dist.mat <- dist(matrix, method = dist_methods[i]) # distance matrix
      cluster <- hclust(dist.mat, method = hclust_methods[j]) # apply method
      ct <- cutree(cluster, k) # k to generate k clusters
      CM <- table(as.factor(dataset$class), as.factor(ct))
      accuracy.CM <- sum(diag(CM))/sum(CM)
      if (accuracy.CM > better.accuracy){
        better.accuracy <- accuracy.CM
        better <- c(dist_methods[i], hclust_methods[j], accuracy.CM)
      }
    }
  }
  return(better)
}
```
## **Comienzo de Actividades**

## Archivo *h.csv*

```{r}
## Exploratory Analysis
guess <- read.csv("guess.csv")
summary(guess)

```

Este conjunto de datos tiene la particularidad que no posee columna de clase, de tal manera que no se podrán hacer matrices de confusión para evaluar y comparar modelos. 
Un análisis exploratorio detallado de su _summary_ y de su gráfico será en este escenario la única forma de evaluación de los modelos a aplicar.

El gráfico de los cluster se muestra a continuación.

```{r}
names(guess)[1] <- "x"
names(guess)[2] <- "y"
plot(guess$x, guess$y, main = "Vista Previa")

```


Se pueden ver a simple vista al menos dos clusters bien separados, uno en la parte inferior izquierda y otro en la parte superior derecha, pero también puede apreciarse que el cluster más grande podría sub-dividirse en algunos mas.

Para este caso se utilizará un método llamado **"Codo de Jambú"** o **"Método de Jambú"** el cual nos permite ver cuántos clusters pueden ser seleccionados a través de una gráfica producida por la minimización de la suma sucesiva de los errores estándar de las iteraciones de __k__ centroides. Nótese que al mencionar _centroide_ se hace referencia al uso del algoritmo de __K-medias__ como ayuda para la realización de los cálculos.

La seleccion de los cluster básicamente se hace cuando la gráfica llegue a cierto punto lo suficientemente bajo y que suavice la curva descendiente, lo cual indica que se ha seleccionado la iteración con menor error.


A continuación se procede a graficar la curva producida por el **Método de Jambú** para el dataset __guess.csv__.

```{r}
c.jambu(guess)

```

En la gráfica puede apreciarque la curva empieza a suavizarse y variar de manera de forma mínima, cuando el eje con el nombre **Number of Clusters** esta aproximadamente entre 5 y 6. Se ha decidido tomar el número 5, ya que es el que visualmente se parece mas a la cantidad de clusters existentes en la gráfica.

A continuación se Utilizarán Kmedias, PAM y 6 variaciones de Hclust entre las distancias _Euclidean_ y _Manhattan_ con los métodos de Hcluster más conocidos _Single_, _Complete_ y _Average_. Todos ellos con K = 5, que fue el número seleccionado a través del **Método de Jambú**.

### K-medias
```{r, echo=F}
## K means
guess.kmeans <- kmeans(guess[,c("x", "y")], centers = 5)

plot(guess$x, guess$y, col= guess.kmeans$cluster, main = "K-means")
points(guess.kmeans$centers[, c("x", "y")],
       col=1:5,
       pch = 19,
       cex = 3)

```

Con Kmedias se hace una buena división de los 5 clusters que pudieran observarse si se presta atención al cluster inicial. Se observa una buena ubicación de los centroides y un mínimo nivel de solapamiento entre algunos elementos del conjunto.


### Partitioning Around Medioids (PAM)
```{r}
### Partitioning Around Medioids (PAM)
guess.pam <- pam(guess[,1:2], 5)

plot(guess$x, guess$y, col = guess.pam$clustering, main = "PAM")
points(guess.pam$medoids,
       col=1:5,
       pch = 18,
       cex = 3)

```

Para PAM se observa que las clases se distribuyen muy parecido a Kmedias. Se observa un poco menos de solapamiento que en Kmedias. 

### Clustering Jerárquico
```{r, echo=F}
## Hcluster
guess.num <- guess # a copy of the dataframe
guess.num <- as.matrix(guess.num) # convert into a matrix
```

Se procede a realizar las combinaciones previamente descritas en el algoritmo de clustering Jerárquico.

#### Euclidean - Single 
```{r, echo=F}
# Calculating hclust methods
######### Euclidean - Single 
guess.dist.mat <- dist(guess.num, method = dist_methods[1]) # distance matrix
guess.cluster <- hclust(guess.dist.mat, method = hclust_methods[2]) # apply method
guess.ct <- cutree(guess.cluster, k = 5) # to generate k clusters

dendrogram <- as.dendrogram(guess.cluster)
plot(dendrogram)
rect.hclust(guess.cluster, k = 5, border = c("red"))
plot(guess$x, guess$y, col= guess.ct, main = "HCluster (Euclidean - Single)")
```

Los dendrogramas usando single usualmente tienen forma escalonada, en este caso no nos brinda información útil de la separación de los 5 clusters. En el gráfico de dispersión se puede observar que solo clusterizó en aproximadamente el 99% todo en un solo grupo, dejando a puntos individuales pertenecer a clusters enteros. Mal método.


#### Euclidean - Complete
```{r, echo=F}
########## Euclidean - Complete (good enought)
guess.dist.mat <- dist(guess.num, method = dist_methods[1]) # distance matrix
guess.cluster <- hclust(guess.dist.mat, method = hclust_methods[3]) # apply method
guess.ct <- cutree(guess.cluster, k = 5) # to generate k clusters

dendrogram <- as.dendrogram(guess.cluster)
plot(dendrogram)
rect.hclust(guess.cluster, k = 5, border = c("red"))
plot(guess$x, guess$y, col= guess.ct, main = "HCluster (Euclidean - Complete)")
```



#### Euclidean - Average 
```{r, echo=F}
############ Euclidean - Average (so so bad)
guess.dist.mat <- dist(guess.num, method = dist_methods[1]) # distance matrix
guess.cluster <- hclust(guess.dist.mat, method = hclust_methods[4]) # apply method
guess.ct <- cutree(guess.cluster, k = 5) # to generate k clusters

dendrogram <- as.dendrogram(guess.cluster)
plot(dendrogram)
rect.hclust(guess.cluster, k = 5, border = c("red"))
plot(guess$x, guess$y, col= guess.ct, main = "HCluster (Euclidean - Average)")
```


#### Euclidean - Single 
```{r, echo=F}
############# Manhattan - Single (BAD)
guess.dist.mat <- dist(guess.num, method = dist_methods[3]) # distance matrix
guess.cluster <- hclust(guess.dist.mat, method = hclust_methods[2]) # apply method
guess.ct <- cutree(guess.cluster, k = 5) # to generate k clusters

dendrogram <- as.dendrogram(guess.cluster)
plot(dendrogram)
rect.hclust(guess.cluster, k = 5, border = c("red"))
plot(guess$x, guess$y, col= guess.ct, main = "HCluster (Manhattan - Single)")
```


#### Euclidean - Single 
```{r, echo=F}
############ Manhattan - Complete
guess.dist.mat <- dist(guess.num, method = dist_methods[3]) # distance matrix
guess.cluster <- hclust(guess.dist.mat, method = hclust_methods[3]) # apply method
guess.ct <- cutree(guess.cluster, k = 5) # to generate k clusters

dendrogram <- as.dendrogram(guess.cluster)
plot(dendrogram)
rect.hclust(guess.cluster, k = 5, border = c("red"))
plot(guess$x, guess$y, col= guess.ct, main = "HCluster (Manhattan - Complete)")
```


#### Euclidean - Single 
```{r, echo=F}
########### Manhattan - Average (so so BAD)
guess.dist.mat <- dist(guess.num, method = dist_methods[3]) # distance matrix
guess.cluster <- hclust(guess.dist.mat, method = hclust_methods[4]) # apply method
guess.ct <- cutree(guess.cluster, k = 5) # to generate k clusters

dendrogram <- as.dendrogram(guess.cluster)
plot(dendrogram)
rect.hclust(guess.cluster, k = 5, border = c("red"))
plot(guess$x, guess$y, col= guess.ct, main = "HCluster (Manhattan - Average)")
```


































































































