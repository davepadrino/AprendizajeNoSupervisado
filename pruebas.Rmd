---
title: "Tarea 3"
author: "David Padrino"
date: "Domingo, 10 de abril de 2016"
widescreen: yes
runtime: shiny
output: html_document
---
# Introducción
Primero que nada se debe establecer el _location directory_ como la ubicación actual del archivo.
Se instalan e invocan las bibliotecas necesarias, se leen los archivos a medida que se va trabajando con ellos


# Puesta a punto
## 1 - Se cargan las bibliotecas necesarias
```{r setup, warning=FALSE, message=FALSE}  
# install.packages("clue")
# install.packages("Rcmdr")
# install.packages("rglwidget")
library("stringr")
library("caret")
library("clue")
library("cluster")
library("rgl")
library("knitr")
library("rglwidget")
knit_hooks$set(webgl = hook_webgl)
```


## 3 - Se crearon funciones personalizadas
Algunas de ellas son la implementación del codo de jambú, calculo de kmedias para 2D y 3D, comparación de Kmedias y PAM a través de sus matries de confusión, etc.
```{r, echo=FALSE}
#Set classes after 1
set_class = function(dataset){
  dataset$class <-as.numeric(dataset$class)
  dataset$class <- dataset$class + 1
  return(dataset)
}

# codo de jambu 
c.jambu= function(d){
  mydata <- d
  wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
  for (i in 2:15) wss[i] <- sum(kmeans(mydata, centers=i)$withinss)
  plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
}

# Calculating best kmeans algorithm 2D
k.means2D = function(dataset, centers){
  kmeans.accuracy.CM <- 0
  kmeans.better.accuracy.CM <- 0
  for (i in 1:10){
    kmeans <- kmeans(dataset[,c("x", "y")], centers = centers)
    kmeans.CM <- table(kmeans$cluster, dataset$class)
    kmeans.accuracy.CM <- sum(diag(kmeans.CM))/sum(kmeans.CM)
    if (kmeans.accuracy.CM > kmeans.better.accuracy.CM){
      kmeans.better <- kmeans
      kmeans.better.accuracy.CM <- kmeans.accuracy.CM # useless (?)
    }
  }
  return(kmeans.better)
}


# Calculating best kmeans algorithm 2D
k.means3D = function(dataset, centers){
  kmeans.accuracy.CM <- 0
  kmeans.better.accuracy.CM <- 0
  for (i in 1:10){
    kmeans <- kmeans(dataset[,c("x", "y","z")], centers = centers)
    kmeans.CM <- table(kmeans$cluster, dataset$class)
    kmeans.accuracy.CM <- sum(diag(kmeans.CM))/sum(kmeans.CM)
    if (kmeans.accuracy.CM > kmeans.better.accuracy.CM){
      kmeans.better <- kmeans
      kmeans.better.accuracy.CM <- kmeans.accuracy.CM # useless (?)
    }
  }
  return(kmeans.better)
}


# Comparing kmeans vs PAM
compare.kmeans.pam = function(kmeans.acc, pam.acc){
  kmeans.vs.pam <- 0
  if (kmeans.acc >= pam.acc){
    #a.kmeans.vs.pam <- kmeans.acc
    kmeans.vs.pam <- c("kmeans", kmeans.acc)
    return(kmeans.vs.pam) 
  }else{
    #a.kmeans.vs.pam <- pam.acc
    kmeans.vs.pam <- c("pam", pam.acc)
    return(kmeans.vs.pam) 
  }
}

# Calculating best K for good_luck.csv
good_luck.kmeans.fun <- function(array, good_luck){
  better.kmeans <- 0
  for (i in 2:length(array)){
    good_luck.kmeans <- kmeans(good_luck[,array], centers = i)
    good_luck.kmeans.CM <- table(good_luck.kmeans$cluster, good_luck$class)
    good_luck.kmeans.accuracy <- sum(diag(good_luck.kmeans.CM))/sum(good_luck.kmeans.CM)
    if (good_luck.kmeans.accuracy > better.kmeans){
      better.kmeans <- good_luck.kmeans.accuracy
      fin <- c(better.kmeans, i)
    }
  }
  return(fin)
}


# Calculating each distance method vs each hclust method 
match.hclust = function(matrix, k, dataset){
  better.accuracy <- 0
  for (i in 1:length(dist_methods)){
    for (j in 1:length(hclust_methods)){
      dist.mat <- dist(matrix, method = dist_methods[i]) # distance matrix
      cluster <- hclust(dist.mat, method = hclust_methods[j]) # apply method
      ct <- cutree(cluster, k) # k to generate k clusters
      CM <- table(as.factor(dataset$class), as.factor(ct))
      accuracy.CM <- sum(diag(CM))/sum(CM)
      if (accuracy.CM > better.accuracy){
        better.accuracy <- accuracy.CM
        better <- c(dist_methods[i], hclust_methods[j], accuracy.CM)
      }
    }
  }
  return(better)
}


euc.distance <- function(x1, y1, x2, y2){
  return( sqrt( ( (x1 - x2) ^ 2) + ( (y1 - y2) ^ 2) ) )
} 

do.distance.matrix <- function(dataframe, dist.matrix, cent){
  for (i in 1:nrow(cent))
    dist.matrix[,i] <- euc.distance(cent[i,1], cent[i,2], dataframe[,1], dataframe[,2])
  return(dist.matrix)
}

myKmeans <- function(dataframe, k, nItter, centroids = NULL){
  if(is.null(centroids)){
    rand.centroids <- dataframe[sample(nrow(dataframe), k), ][-3]  
    centroids <- matrix(0, nrow=k, ncol=2)
    centroids[,1] <- rand.centroids[,1]
    centroids[,2] <- rand.centroids[,2]
  }
  cluster <- vector(mode="list", length = nItter)
  center <- vector(mode="list", length = nItter)
  clusters <- vector(mode = "numeric", length = nrow(dataframe) )
  distsToCenters <- matrix(0, nrow = nrow(dataframe), ncol = nrow(centroids))
  n.clust <- vector("numeric", 3)
  n.clust.aux <- vector("numeric", 3)
  for(i in 1:nItter) {
    distsToCenters <- do.distance.matrix(dataframe, distsToCenters,centroids) # distance matrix
    clusters <- apply(distsToCenters, 1, which.min) # clusterize
    centroids <- apply(dataframe, 2, tapply, clusters, mean)
    if (i>1){
      for(j in 1:length(n.clust)){
        n.clust.aux[j] <- length(clusters[clusters == j])
      }
      if((n.clust[1] == n.clust.aux[1]) && (n.clust[2] == n.clust.aux[2]) && (n.clust[3] == n.clust.aux[3])){ # if there are no changes, return list
        cluster <- clusters
        center <- centroids
        return(list(centroids = center, clusters = cluster))
      }else{
        n.clust[1] = n.clust.aux[1]
        n.clust[2] = n.clust.aux[2]
        n.clust[3] = n.clust.aux[3]
      }
    }
  }
}
```


## **Comienzo de Actividades**

## **Archivo *a_big.csv* **

```{r}
## Exploratory Analysis
a_big <- read.csv("a_big.csv")
summary(a_big)
```

Este es el último de los datasets que tiene su última columna con números enteros, los cuales pueden ser utilizados como clase de los grupos.
Se utiliza la función _set class_ previamente creada para establecer los valores de la ultima columa (entre 0 y 2) entre 1 y 3.
El gráfico de los cluster se muestra a continuación.

```{r}
names(a_big)[1] <- "x"
names(a_big)[2] <- "y"
names(a_big)[3] <- "class"
a_big <- set_class(a_big)
plot(a_big$x, a_big$y, col=a_big$class, main = "Vista Previa") 
```

Se puede observar la presencia de tres clusters bien definidos, por lo que se utilizará __K=3__ para la realización de los algoritmos.

__Observación__
En la Introducción de este trabajo se realizó la presentación de un marco teórico que hace referencia a los elementos  y herramientas usadas en esta tarea. En ella se mencionó los pro y contra del algoritmo Hclust, el cual calcula una matriz de confusión para realizar su tarea. En este caso no se puede utililizar este algoritmo con el dataset completo.

Se procede a probar su funcionamiento con Kmedias


### K-medias
```{r, echo=F}
## K means
a_big.kmeans <- k.means2D(dataset = a_big, centers = 3)
a_big.kmeans.CM <- table(a_big.kmeans$cluster, a_big$class)
a_big.kmeans.accuracy <- sum(diag(a_big.kmeans.CM))/sum(a_big.kmeans.CM)

plot(a_big$x, a_big$y, col= a_big.kmeans$cluster, main = "K-means")
points(a_big.kmeans$centers[, c("x", "y")],
       col=0,
       pch = 19,
       cex = 2)
```


Para K-medias se observa un desempeño excelente clasificando de manera casi ideal cada punto alrededor de su centroide. Además la tasa de acierto de su matriz de confusión confirma lo bien que se adapta este algoritmo al modelo, específicamente: 

```{r, echo=F,} 
a_big.kmeans.accuracy
```

Se puede observar que al momento de aplicar Kmedias aparece en el nombre de la variable __a_big.kmeans__ (la utilizada para almacenar el resultado de la aplicación de kmedias) el nombre __Large Kmeans__junto con un tamaño en Megabytes, lo que indica que se está trabajando con un dataset de gran tamaño para __R__.


Se procederá a realizar una implementación del algoritmo de Kmedias de forma de obtener unos buenos centroides con una muestra aceptable del  dataset. Una vez se tengan los centroides (calculados con un _sample_ del total) se probará el funcionamiento de los mismos en la muestra total.

Para realizar una repartición equitativa se calcula la probabilidad de cada elemento para ser seleccionado dependiendo de la clase a la que pertenezca. Hecho esto se agrega en una posicion de un vector que se utilizará en la funcion __sample__ cuando se realice la selección de elementos aleatoria como muestra. La cual será el 10% de la muestra, es decir, aproximadamente 3000 elementos.

```{r, echo= F}
prob_vec = vector(mode = "numeric", length = nrow(a_big))
prob_vec[a_big[,"class"] == 1] = 1/sum(a_big[, "class"] == 1)
prob_vec[a_big[,"class"] == 2] = 1/sum(a_big[, "class"] == 2)
prob_vec[a_big[,"class"] == 3] = 1/sum(a_big[, "class"] == 3)

a_big.sample = sample(x = nrow(a_big)
                      , size = floor(nrow(a_big) * 0.15)
                      , prob = prob_vec
                      , replace = F)
a_big.subset <- a_big[a_big.sample, ]
a_big.subset.kmeans <- myKmeans(a_big.subset, k = 3, nItter = 10)
```


La visualización de la muestra con los centroides y clusters calculados sería de la siguiente forma:


```{r}
plot(a_big.subset$x, a_big.subset$y, col = a_big.subset.kmeans$clusters, main = " Kmedias Personalizado ")
points(a_big.subset.kmeans$centroids[,c("x", "y")],
       col = 0,
       pch = 19,
       cex = 2)
```

Una vez realizado esto, se procede a probar en el dataset completo los nuevos centroides y clusters.

```{r}
a_big.final = myKmeans(dataframe = a_big, k = 3, nItter = 10, centroids = a_big.subset.kmeans$centroids)
a_big.final.CM <- table(a_big.final$cluster, a_big$class)
a_big.final.kmeans.accuracy <- sum(diag(a_big.final.CM))/sum(a_big.final.CM)

plot(a_big$x, a_big$y, col = a_big.final$clusters, main = " Kmedias Personalizado FINAL")
points(a_big.final$centroids[,c("x", "y")],
       col = 0,
       pch = 19,
       cex = 2)
```

Se puede observar que los centroides (de blanco para poder diferenciar) fueron colocados de una buena manera a vista superficial por lo que se puede decir que el algoritmo tuvo un buen desempeño.

Además de lo anterior dicho los tiempos de respuestas fueron mucho menores que al momento de hacer kmedias directamente con todo el set de datos lo que supone que una buena estrategia para tratar los grandes sets de datos es eligiendo una muestra aceptable para obtener clusters y centroides y luego con ellos obtenidos hacer el cálculo con todo el dataset.


Su tasa de aciertos quizas no es la mejor, esto debido a que quizas con el _samplig_ se haya perdido algo de información
```{r}
a_big.final.kmeans.accuracy
```





