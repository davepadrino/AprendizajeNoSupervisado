---
title: "h.csv"
author: "David Padrino"
date: "April 10, 2016"
output: html_document
---
# Puesta a punto
## 1 - Se cargan las bibliotecas necesarias
Entre ellas, existen bibliotecas que permiten interactuar con objetos, rotar y mover gráficos en 3D, por ejemplo.
```{r setup, message=FALSE, warning=FALSE}  
# install.packages("clue")
# install.packages("Rcmdr")
# install.packages("rglwidget")
library("stringr")
library("caret")
library("clue")
library("cluster")
library("rgl")
library("knitr")
library("rglwidget")
knit_hooks$set(webgl = hook_webgl)
```

## 2 - Se procede a crear 2 arreglos con los metodos necesarios para realizar el clustering jerárquico. 
Se aclara que **para todos los metodos donde se usó clustering jerárquico se utilizó una función personalizada que selecciona la mejor combinación de los métodos tanto de distancia como de clustering expuestos en los arreglos debajo de este párrafo, basado en la tasa de aciertos de su matriz de confusión**
Los resultados de las matrices de confusión vendrán dados entre 0 y 1.




```{r}
hclust_methods <- c("ward.D", "single", "complete", "average", "mcquitty", "median", "centroid", "ward.D2")
dist_methods <- c("euclidean", "maximum", "manhattan", "minkowski")
```

## 3 - Se crearon ciertas funciones por comodidad, como la implementación del codo de jambú, calculo de kmedias para 2D y 3D, comparación de Kmedias y PAM a través de sus matries de confusión, etc.

```{r, echo=FALSE}
#Set classes after 1
set_class = function(dataset){
  dataset$class <-as.numeric(dataset$class)
  dataset$class <- dataset$class + 1
  return(dataset)
}

# codo de jambu 
c.jambu= function(d){
  mydata <- d
  wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
  for (i in 2:15) wss[i] <- sum(kmeans(mydata, centers=i)$withinss)
  plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
}

# Calculating best kmeans algorithm 2D
k.means2D = function(dataset, centers){
  kmeans.accuracy.CM <- 0
  kmeans.better.accuracy.CM <- 0
  for (i in 1:10){
    kmeans <- kmeans(dataset[,c("x", "y")], centers = centers)
    kmeans.CM <- table(kmeans$cluster, dataset$class)
    kmeans.accuracy.CM <- sum(diag(kmeans.CM))/sum(kmeans.CM)
    if (kmeans.accuracy.CM > kmeans.better.accuracy.CM){
      kmeans.better <- kmeans
      kmeans.better.accuracy.CM <- kmeans.accuracy.CM # useless (?)
    }
  }
  return(kmeans.better)
}


# Calculating best kmeans algorithm 3D
k.means3D = function(dataset, centers){
  kmeans.accuracy.CM <- 0
  kmeans.better.accuracy.CM <- 0
  for (i in 1:10){
    kmeans <- kmeans(dataset[,c("x", "y","z")], centers = centers)
    kmeans.CM <- table(kmeans$cluster, dataset$class)
    kmeans.accuracy.CM <- sum(diag(kmeans.CM))/sum(kmeans.CM)
    if (kmeans.accuracy.CM > kmeans.better.accuracy.CM){
      kmeans.better <- kmeans
      kmeans.better.accuracy.CM <- kmeans.accuracy.CM # useless (?)
    }
  }
  return(kmeans.better)
}


# Calculating each distance method vs each hclust method 
match.hclust = function(matrix, k, dataset){
  better.accuracy <- 0
  for (i in 1:length(dist_methods)){
    for (j in 1:length(hclust_methods)){
      dist.mat <- dist(matrix, method = dist_methods[i]) # distance matrix
      cluster <- hclust(dist.mat, method = hclust_methods[j]) # apply method
      ct <- cutree(cluster, k) # k to generate k clusters
      CM <- table(as.factor(dataset$class), as.factor(ct))
      accuracy.CM <- sum(diag(CM))/sum(CM)
      if (accuracy.CM > better.accuracy){
        better.accuracy <- accuracy.CM
        better <- c(dist_methods[i], hclust_methods[j], accuracy.CM)
      }
    }
  }
  return(better)
}

```
## ** Comienzo de Actividades**

## **Archivo *help.csv* **


```{r, echo=FALSE}
help.clase = function(numero){
  # Selecting 3 clusters
  if(numero < -1.0)
    return(1)
  else if(numero < 7.0)
    return(2)
  else
    return(3)
} 
```


```{r}
## Exploratory Analysis
help <- read.csv("help.csv")
summary(help)
```

Se observa que la ultima columna efectivamente es de números flotantes entre -4.7 y 14 aproximadamente, lo cual no sirve para establecer las clases, se procede a aplicar una función de asignacion de clases, luego a cambiar los nombres de las columnas y finalmente realizar un gráfico con los colores (nuevas clases asignadas con la función de asignación de clases ya pre-establecida) para apreciar mejor la distribucion de los datos

```{r, testgl8, webgl=TRUE}
names(help)[1] <- "x"
names(help)[2] <- "y"
names(help)[3] <- "z"
names(help)[4] <- "class"
plot3d(help$x, help$y, help$z, main = "Vista Previa")

```


Se eligieron **3 clusters** por la forma que presenta el dataset, el cual tuvo que graficarse de cierta forma para poder darle forma similar a dos "S" y una especie de espiral, similares a los datasets _h.csv_ y _s.csv_. Se decidió que cada _"letra"_ formara un cluster aparte.

```{r, testgl81, webgl=TRUE}
for (i in 1:length(help$class)){
  help$class[i] <- help.clase(help$class[i])
}
plot3d(help$x, help$y, help$z, col = help$class, main = "Vista Previa")
```

Los números seleccionados para la regla de asignación de clases fueron elegidos acorde a la distribución de los datos de la última columna (antes de que fuera aplicada la regla), su cuartil 1 y su cuartil 3 especificamente para que fuera separada equitativamente

Al aplicar la __regla de asignacion de clases__ se puede observar que las "S" estan divididas a la mitad y en medio la espiral tiene otras clases.

__El problema__ en este gráfico radica en la asignación irregular de las clases en cada "letra" por lo cual se procederá a realizar una evaluación de acuerdo al eje X para realizar la separación por "letra" como clusters independientes.

Para ello, luego de realizadas varias pruebas, se decidió lo siguiente:

```{r}
help$class = 1
help$class[help$x < 10] = 2
help$class[help$x > 40] = 3
```

Lo anterior, técnicamente hablando, fue primero, asignar __todas__ las clases a una misma _(clase 1)_, luego de ellos, se realizó seleccionó las columna _class_ del dataframe y una vez se filtraron sus columnas menores a 10 en el eje __x__, se les asignó a estas la clase 2.
Mismo procedimiento para la clase 3 pero con los elementos en __x__ mayores a 40.

Es importante recordar, como se mencionó arriba, que los números seleccionados para realizar el filtrado (10 y 40) fueron elegidos a través de pruebas sucesivas observando el comportamiento de la gráfica a medida que se iban colocando valores. Igualmente en la gráfica se aprecia que en el eje __x__ se 0 a 10 aproximadamente pertenecen al primer conglomerado, de 20 a 40 el siguiente y el último de 40 en adelante.

```{r, testgl82, webgl=TRUE}
plot3d(help$x, help$y, help$z, col = help$class, main = "Vista Previa con nuevas clases")
```

Una vez resuelto el inconveniente con la asignación de clases, se procede al cálculo de K-medias y PAM con 3 centros y medioides respectivamente.


### K-medias
```{r, echo=F, testgl9, webgl=TRUE}
## K means
help.kmeans <- k.means3D(dataset = help, centers = 3)
help.kmeans.CM <- table(help.kmeans$cluster, help$class)
help.kmeans.accuracy <- sum(diag(help.kmeans.CM))/sum(help.kmeans.CM)

plot3d(help$x, help$y, help$z, col = help.kmeans$cluster, main = "K-means")
rgl.spheres(help.kmeans$centers[, c("x", "y", "z")], r = 0.8, color = 1:3) 
```
Para K-medias se observa un desempeño casi perfecto clasificando de manera excelente cada punto alrededor de su centroide (y a su vez de cada letra). La tasa de acierto de su matriz de confusión es altísima. Específicamente: 
```{r, echo=F,} 
help.kmeans.accuracy
```


### Partitioning Around Medioids (PAM)
```{r, echo=F, testgl10, webgl=TRUE}
### Partitioning Around Medioids (PAM)
help.pam <- pam(help[,1:3], 3)
help.pam.CM <- table(help.pam$clustering, help$class)
help.pam.accuracy.CM <- sum(diag(help.pam.CM))/sum(help.pam.CM)

plot3d(help$x, help$y, help$z, col = help.pam$clustering, main = "PAM")
rgl.spheres(help.pam$medoids[, c("x", "y", "z")], r = 0.8, color = 1:3) 
```
Para PAM se observa que las clases se distribuyen muy parecido a Kmedias, de muy buena forma alrededor de sus medioides sin embargo se aprecia que su tasa de acierto es bastante baja. Específicamente: 
```{r, echo=F,} 
help.pam.accuracy.CM 
```


### Clustering Jerárquico
```{r, echo=F}
## Hcluster
help.num <- help # a copy of the dataframe
help.num$class <-NULL # Delete class column
help.num <- as.matrix(help.num) # convert into a matrix

# Calculating each distance method vs each hclust method 
help.hclust = match.hclust(help.num, 3, help)

help.dist.mat <- dist(help.num, method = help.hclust[1]) # distance matrix
help.cluster <- hclust(help.dist.mat, method = help.hclust[2]) # apply method
help.ct <- cutree(help.cluster, k =3) # k to generate 3 clusters
help.dendrogram <- as.dendrogram(help.cluster)
plot(help.dendrogram) # dendrogram
rect.hclust(help.cluster, k = 3, border = c("red"))
```
```{r, echo=F, testgl11, webgl=TRUE}
plot3d(help$x, help$y, help$z, col = help.ct, main = "HCluster")

```
Para HCluster se puede ver que clusteriza de una manera diferente a la forma que lo haría Kmedias o PAM, de forma que, si asumimos que cada cluster es una letra ("S", "O" y "S"), no toma cada letra como un cluster sino que toma un par de "letras" como un solo cluster mientras que divide el otro en dos clusters, se puede observar en su dendrograma una estructura bastante similar teniendo en cuenta que el grupo grande es el de las dos "letras" y los pequeños que se dividen en 2, son cada una de las "letras" que fueron separadas en un cluster cada una. Su tasa de aciertos de acuerdo a su matriz de confusión (que se muestra a continuación, también se mostrarán los métodos utilizados por Hclust) resulta en uno de los valores mas altos de los algoritmos anteriormente probados.

Métodos usados
```{r, echo=F}
help.hclust[1]
help.hclust[2]

```

Exactitud:
```{r, echo=F}
help.hclust[3]
```
