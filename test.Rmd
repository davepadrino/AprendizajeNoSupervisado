---
title: "Untitled"
author: "Daveeeeee"
date: "April 8, 2016"
output: html_document
runtime: shiny
---

# Introducción
Primero que nada se debe establecer el _location directory_ como la bicación actual del archivo.
Se instalan e invocan las bibliotecas necesarias, se leen los archivos a medida que se va trabajando con ellos
```{r setup}  
# install.packages("clue")
# install.packages("Rcmdr")
# install.packages("rglwidget")
library("stringr")
library("caret")
library("clue")
library("cluster")
library("rgl")
library("knitr")
library("rglwidget")
knit_hooks$set(webgl = hook_webgl)
```

Se procede a crear 2 arreglos con los metodos necesarios para realizar el clustering jerárquico.

```{r}
hclust_methods <- c("ward.D", "single", "complete", "average", "mcquitty", "median", "centroid", "ward.D2")
dist_methods <- c("euclidean", "maximum", "manhattan", "minkowski")
```

Se crearon ciertas funciones por comodidad, como la implementación del codo de jambú, calculo de kmedias para 2D y 3D, comparación de Kmedias y PAM a través de sus matries de confusión, etc.
```{r, echo=FALSE}
# codo de jambu 
c.jambu= function(d){
  mydata <- d
  wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
  for (i in 2:15) wss[i] <- sum(kmeans(mydata, centers=i)$withinss)
  plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
}

# Calculating best kmeans algorithm 2D
k.means2D = function(dataset, centers){
  kmeans.accuracy.CM <- 0
  kmeans.better.accuracy.CM <- 0
  for (i in 1:10){
    kmeans <- kmeans(dataset[,c("x", "y")], centers = centers)
    kmeans.CM <- table(kmeans$cluster, dataset$class)
    kmeans.accuracy.CM <- sum(diag(kmeans.CM))/sum(kmeans.CM)
    if (kmeans.accuracy.CM > kmeans.better.accuracy.CM){
      kmeans.better <- kmeans
      kmeans.better.accuracy.CM <- kmeans.accuracy.CM # useless (?)
    }
  }
  return(kmeans.better)
}


# Calculating best kmeans algorithm 2D
k.means3D = function(dataset, centers){
  kmeans.accuracy.CM <- 0
  kmeans.better.accuracy.CM <- 0
  for (i in 1:10){
    kmeans <- kmeans(dataset[,c("x", "y","z")], centers = centers)
    kmeans.CM <- table(kmeans$cluster, dataset$class)
    kmeans.accuracy.CM <- sum(diag(kmeans.CM))/sum(kmeans.CM)
    if (kmeans.accuracy.CM > kmeans.better.accuracy.CM){
      kmeans.better <- kmeans
      kmeans.better.accuracy.CM <- kmeans.accuracy.CM # useless (?)
    }
  }
  return(kmeans.better)
}


# Comparing kmeans vs PAM
compare.kmeans.pam = function(kmeans.acc, pam.acc){
  kmeans.vs.pam <- 0
  if (kmeans.acc >= pam.acc){
    #a.kmeans.vs.pam <- kmeans.acc
    kmeans.vs.pam <- c("kmeans", kmeans.acc)
    return(kmeans.vs.pam) 
  }else{
    #a.kmeans.vs.pam <- pam.acc
    kmeans.vs.pam <- c("pam", pam.acc)
    return(kmeans.vs.pam) 
  }
}

# Calculating each distance method vs each hclust method 
match.hclust = function(matrix, k, dataset){
  better.accuracy <- 0
  for (i in 1:length(dist_methods)){
    for (j in 1:length(hclust_methods)){
      dist.mat <- dist(matrix, method = dist_methods[i]) # distance matrix
      cluster <- hclust(dist.mat, method = hclust_methods[j]) # apply method
      ct <- cutree(cluster, k) # k to generate k clusters
      CM <- table(as.factor(dataset$class), as.factor(ct))
      accuracy.CM <- sum(diag(CM))/sum(CM)
      if (accuracy.CM > better.accuracy){
        better.accuracy <- accuracy.CM
        better <- c(dist_methods[i], hclust_methods[j], accuracy.CM)
      }
    }
  }
  return(better)
}
```

## Archivo *h.csv*
```{r, echo=FALSE}
h.clase = function(numero){
  # Selecting 3 clusters
  if(numero < 6.0)
    return(1)
  else if(numero < 8.0)
    return(2)
  else if(numero < 10.0)
    return(3)
  else if(numero < 12.0)
    return(4)
  else
    return(5)
}
```


```{r}
## Exploratory Analysis
h <- read.csv("h.csv")
#dim(h)
names(h)
#str(h)
summary(h)
# c.jambu(h)
```

Se observa que la ultima columna efectivamente es de números enteros lo cual sirve para establecer las clases, se procede a cambiar los nombres de las columnas y a realizar un gráfico con los colores de los elementos para apreciar mejor la distribucion de los mismos.

```{r, testgl1, webgl=TRUE}

names(h)[1] <- "x"
names(h)[2] <- "y"
names(h)[3] <- "z"
names(h)[4] <- "class"
for (i in 1:length(h$class)){
  h$class[i] <- h.clase(h$class[i])
}

plot3d(h$x, h$y, h$z, col = h$class, main="Vista Previa")


```


Se eligieron 5 clusters por la forma del dataset, a medida que el "espiral" se va "desenrollando" da la impresion que tambien se van separando los puntos del conglomerado inicial
Además se eligio un numero entero menor y mas cercano al primer cuartil, mismo para el 3er cuartil

Al apreciar el gráfico se prueba con 5 centroides.
Luego se procede al cálculo de K-medias y PAM respectivamente 

Ambos algoritmos se corrieron con 5 centros y medioides respectivamente.

Los resultados de las matrices de confusion se hacen entre PAM y K-medias de forma que el "ganador" se compare con el mejor algoritmo de cluster jerárquico.

```{r, echo=F}
## K means
h.kmeans <- k.means3D(dataset = h, centers = 5)
h.kmeans.CM <- table(h.kmeans$cluster, h$class)
h.kmeans.accuracy <- sum(diag(h.kmeans.CM))/sum(h.kmeans.CM)


## Partitioning Around Medioids (PAM)
h.pam <- pam(h[,1:3], 5)
h.pam.CM <- table(h.pam$clustering, h$class)
h.pam.accuracy.CM <- sum(diag(h.pam.CM))/sum(h.pam.CM)

#### Comparison of kmean's centroids vs pam's medioids
h.kmeans.vs.pam <- compare.kmeans.pam(h.kmeans.accuracy, h.pam.accuracy.CM)

# Medioids are part of the DataSet, centroids aren't necessarily
```
Luego se realiza el clustering jerárquico. Se utilizaron los arreglos de los métodos para hacer un amtch y asi elegir el mejor de ellos.


```{r, echo=F}
## Hcluster
h.num <- h # a copy of the dataframe
h.num$class <-NULL # Delete class column
h.num <- as.matrix(h.num) # convert into a matrix

# Calculating each distance method vs each hclust method 
h.hclust = match.hclust(h.num, 5, h)
```

Se realiza la comparación y el resultante se muestra a continuación
```{r,  echo=F, testgl, webgl=TRUE}
# Comparison hclust vs kmeans.vs.pam
if (h.hclust[3] > h.kmeans.vs.pam[2]){
  #h.final.cluster <- h.hclust.better.accuracy
  h.dist.mat <- dist(h.num, method = h.hclust[1]) # distance matrix
  h.cluster <- hclust(h.dist.mat, method = h.hclust[2]) # apply method
  h.ct <- cutree(h.cluster, k =5) # k to generate 3 clusters
  h.dendrogram <- as.dendrogram(h.cluster)
  plot(h.dendrogram) # dendrogram
  rect.hclust(h.cluster, k = 5, border = c("cyan"))
  h.corte <- cut(h.dendrogram, h=16)$upper # $upper to get useful information instead a forest
  plot(h.corte)
  #rgl.open()
  #rgl.bg(color = "white") # Setup the background color
  plot3d(h$x, h$y, h$z, col = h.ct, main = "HCluster")
}else{
  #h.final.cluster <- h.kmeans.vs.pam
  if (h.kmeans.vs.pam[1] == 'pam'){
    #rgl.open()
    #rgl.bg(color = "white") # Setup the background color
    plot3d(h$x, h$y, h$z, col = h.pam$clustering, main = "PAM")
    rgl.spheres(h.pam$medoids[, c("x", "y", "z")], r = 0.4, color = 1:5) 
  }else{
    #rgl.open()
    #rgl.bg(color = "white") # Setup the background color
    plot3d(h$x, h$y, h$z, col = h.kmeans$cluster, main = "K-means")
    rgl.spheres(h.kmeans$centers[, c("x", "y", "z")], r = 0.4, color = 1:5) 
  }
}

```






































































































